{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out의 효과는 많은 연구 팀에 자극을 주었으며, ZFNet 개발자로 유명한 뉴욕대의 Matthew Zeiler와 Rob Fergus도 그들 중 하나이다. <br /> \n",
    "이들은 “Stochastic Pooling for Regularization of Deep Convolutional Neural Networks”라는 논문을 2013년에 발표하였으며, Drop out이 <br />\n",
    "주로 FC 레이어에 적용하여 효과를 얻었던 것과 달리, pooling 레이어에 stochastic pooling 방법을 적용하여 큰 효과를 얻었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 CNN에서는 pooling이라고 부르는 sub-sampling 을 이용해 feature-map의 크기를 줄이고, 위치나 이동에 좀 더 강인한 성질을 갖는 특징(feature)을 추출할 수 있게 된다. <br />\n",
    "그동안 pooling window에서 최대값을 선택하는 max-pooling 방식이나, window 영역에서 평균 값을 취하는 average pooling 방식이 주로 쓰였다. <br /> \n",
    "하지만 위 pooling 방식을 사용하게 되면 약간의 문제가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window 영역에서 평균 값을 취하는 average pooling 방식을 사용하면, deep CNN에서는 성능이 그리 좋지 못하다. <br /> \n",
    "활성화 함수로 ReLU를 많이 사용하는데, ReLU 특성에 의해 0이 많이 나오게 되면 평균 연산에 의해 강한 자극이 영향이 줄어드는 효과(down-scale weighting)가 발생한다. <br />\n",
    "활성화 함수로 tanh를 사용하게 되면 결과가 더 나빠질 가능성이 있는데, 그 이유는 강한 양의 자극과 음의 자극에 대한 평균을 취하게 되면 서로 상쇄될 수도 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 Max-pooling 방식을 사용하면, 앞서 살펴본 average pooling 같은 문제는 없지만, 학습 데이터에 과적합되기 쉽다. <br />\n",
    "Stochastic pooling은 max-pooling의 장점은 유지하면서 overfitting 문제를 해결하기 위한 방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Pooling (학습 시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic pooling은 max-pooling과 average pooling의 문제점을 해결하기 위해 고안이 된 방법이다. <br />\n",
    "단순하게 최대 activation을 선택하거나 모든 activation의 평균을 구하는 것이 아니라, Drop out과 마찬가지로 확률 $p$에 따라 적절한 activation을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 확률을 구하기 위해 pooling window에 있는 모든 activation의 합을 구한 후, 그 합으로 각각의 activation을 나눠주는 방식(즉, normalization)으로 확률을 구하며, <br /> \n",
    "그 식은 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=200 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 확률을 구한 후 그 영역을 대표하는 activation은 확률에 따라 선정하게 되며, 식은 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 선택을 하게 되면, max-pooling의 효과를 그대로 유지를 하면서, stochastic component를 이용해 과적합을 피할 수 있게 된다. <br /> \n",
    "Max-pooling은 가장 강한 자극만을 선택할 뿐이지만, 경우에 따라서는 최대값이 아니더라도 더 중요한 정보를 갖고 있을 수 있으며, stochastic pooling을 <br /> \n",
    "쓰게 되면 이것이 가능해진다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림에서 pooling window에는 max-pooling 방식을 사용하면 2.4를 선택해야 하겠지만, stochastic pooling 방식을 사용하면 1.6을 선택할 수도 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약에 average pooling을 선택한다면, 4/9 = 0.444가 되어 의미 없는 작은 값이 나오게 된다. <br />\n",
    "Back-propagation을 할 때는 max-pooling 방식과 마찬가지로 선정된 activation만 남기고 나머지는 전부 0으로 한 후 처리한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이해를 돕기 위해, 3x3 pooling 윈도우의 activation이 아래와 같이 된 경우를 고려해보자.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=400 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터를 기반으로 확률을 구하면 아래 그림의 왼쪽과 같으며, 이것은 크기에 따라 sorting을 하면 오른쪽과 같은 결과가 나온다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약에 stochastic pooling을 위해 5번을 선택하면, activation은 1.2가 선택이 된다. 위 예에서 activation 값이 1.5인 값이 2개가 있는데, <br />\n",
    "이것은 1번이나 2번을 선택하면, 나오게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Weighting (테스트 시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 마치고 실제로 적용을 할 때도 stochastic pooling을 사용하면 성능이 떨어지는 경향이 있기 때문에, 실제 Test 시에는 다른 방법을 사용한다. <br /> \n",
    "실제로 적용할 때는 아래 식처럼, 각각의 activation에 확률을 weighting factor로 곱해주고 전체를 더하는 방법을 취한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것을 “probabilistic weighting”이라고 부른다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법을 사용하면, pooling window에 있는 각각의 activation이 다른 weighting factor를 곱하기 때문에 average pooling과 다르다. <br />\n",
    "논문의 저자들은 probabilistic weighting을 통해, 마지 drop out이 test 시에 modeling averaging을 통해 성능을 끌어 올리는 것과 비슷한 효과를 얻을 수 있다고 주장한다. <br /> \n",
    "학습을 시킬 때는 확률에 기반한 sampling 방식을 통해 다양한 model을 학습하고, test 시에는 probabilistic weighting을 통한 model averaging을 통해 <br /> \n",
    "성능을 올린다는 점은 drop out의 개념과 거의 유사하다. <br />\n",
    "실제로 test 시에 stochastic pooling과 probabilistic weighting을 비교하는 실험을 수행했는데 probabilistic weighting을 사용하는 것이 더 좋은 결과를 나타냈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Pooling의 성능 실험 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능을 확인하기 위한 실험을 drop out과 마찬가지로 SVHN, MNIST, CIFAR-10, CIFAR-100 등에 대하여 실험을 하였다. <br /> \n",
    "단, 데이터를 그대로 사용하는 것이 아니라, 성능 향상과 빠른 학습을 위해 SVHN, CIFAR-10, CIFAR-100은 전체 이미지의 평균을 빼주고 처리를 하였으며, <br /> \n",
    "MNIST 데이터는 BW(Black White) 이미지이기 때문에 평균을 빼주면 color shift가 많이 일어나 local contrast normalization만을 수행하여 처리하였다. <br /> \n",
    "아래 그림에서 윗줄에 있는 그림은 원래 학습 데이터고 아랫줄에 있는 그림은 전처리를 한 것이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=700 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic pooling의 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 CIFAR-10 데이터에 대하여 average, max, stochastic pooling을 적용하여 각각 실험을 한 결과는 아래 그래프와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그래프를 자세히 보면, 학습 결과는 max-pooling이 가장 좋다. <br /> \n",
    "하지만 test 결과를 보면 max와 average pooling은 과적합이 일어나는 것으로 나타난다. <br /> \n",
    "학습 시 training error는 계속 감소 하지만, test error는 거의 변화가 없다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 stochastic pooling은 training error는 max-pooling보다 높게 나오지만, test error는 학습을 계속 시키면 계속 error가 줄어드는 것을 확인할 수 있다. <br /> \n",
    "즉, 과적합 문제에서 max나 average pooling보다 자유롭다는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 280 epoch 동안 학습을 시키고 결과를 확인해보자. <br /> \n",
    "아래 표와 같이 stochastic pooling을 사용하면 다른 어떤 경우보다 결과가 좋게 나옴을 수치적으로 확인할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=700 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터를 이용한 실험에서도 비슷한 결과를 얻었으며, 결과는 아래 표와 같다. <br /> \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic distortion을 사용하면 학습 데이터를 지능적으로 늘리는 data augmentation 기법을 적용한 것이라서 이와 직접적으로 비교를 하는 것은 공정하지 못하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-100, SVHN 어느 경우나 stochastic pooling의 결과가 가장 좋았으며, 이것으로 stochastic pooling은 확실히 과적합을 피할 수 있는 pooling 방식이 입증되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model averaging의 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model averaging의 효과를 입증하기 위해, MNIST 데이터에 대하여 training과 test 시에 각각의 pooling 방법을 섞어 실험을 하였는데 그 결과는 아래 표와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 결과를 보면, 굵은 글씨의 결과가 성능이 좋다는 것을 확인할 수 있다. <br />\n",
    "학습 시에 stochastic pooling을 사용하고, 테스트 시에 probabilistic weight을 사용했을 때 15.2%로 아주 좋은 성능이 나왔다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 표에서 stochastic-10과 stochastic-100은 실제로 model averaging의 효과를 확인하기 위해 stochastic pooling을 이용한 10개 100개 모델을 <br /> \n",
    "테스트 시에 만들어 실험을 한 결과를 앙상블(ensemble)하였으며, 뒤에 붙는 숫자가 클수록 결과가 좋아진다. <br /> \n",
    "하지만 모델 수가 커질수록 연산 시간이 늘어나는 것을 감수해줘야 한다. <br />\n",
    "100개 모델에 대하여 test에 stochastic pooling을 적용하고 voting을 한 결과가 15.12%가 나왔으며 이것은 test에 probabilistic weighting을 사용한 결과보다는 약간 좋다. <br />\n",
    "하지만 다른 뜻으로 해석하면 test 시에 probabilistic weighting을 사용하면 model averaging의 효과도 얻을 수 있고, 연산 시간도 매우 짧게 할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
