{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over fitting의 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over fitting은 아래의 그림처럼, 학습 데이터에 지나치게 과적합되어 실제 테스트 데이터에서는 결과가 나쁘게 나오는 현상을 말한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약에 동일한 점이 주어지고 그 점들을 대표할 수 있는 함수(곡선)을 추정하는 경우를 생각해보자. <br />\n",
    "만약 가운데가 적절한 추정이라고 한다면 왼쪽 그림은 지나친 단순화를 통해 에러가 많이 발생하는 경우로 under fitting이라고 한다. <br />\n",
    "오른쪽은 점들을 너무 정확하게 표현한 나머지 학습 데이터에 대한 정확도는 좋지만 실제 테스트에서는 에러가 발생할 수 있는 상황이며 over fitting에 해당한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over fitting을 표현하는 적절한 그래프 중 하나는 아래 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터에 대한 학습 결과는 계속 좋아지지만, 테스트 데이터를 이용한 결과는 더 이상 개선이 없는 상황을 Over fitting이라고 한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over fitting에 대한 해결책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over fitting에 대한 해결책으로는 Regularization, Data augmentation을 들 수 있다. <br />\n",
    "Regularization은 학습 시 지나치게 학습 데이터에 집중하는 것을 피하기 위해 일종의 Penalty를 부여하는 방법이다. <br />\n",
    "대표적으로는 ${L}_{1}$/${L}_{2}$-regularization 방법을 들 수가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 양이 적은 경우, 적은 학습 데이터로 인해 일반화의 특성이 떨어지기 때문에 지능적으로 학습 데이터를 늘리는 방식이 많이 적용되고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out은 2012년에 발표된 일종의 Regularization 방법이며, 매우 좋은 결과를 보인다. <br />\n",
    "하지만 2015년에 발표된 Batch Normalization의 부수적인 효과 중 하나가 Regularization 효과이기 때문에 BN 연구자들은 Drop out이 없더라도 성능이 충분히 <br /> \n",
    "잘 나온다고 주장하고 있어 Drop out을 반드시 적용해야하는지 여부는 각 설계자들의 결정에 달려있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out 관련 논문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out은 캐나다 토론토 대학교 팀에서 발표한 알고리즘이며, 이들은 유명한 AlexNet에 실제로 Dropout을 적용하여 성능을 개선을 입증하였다. <br />\n",
    "2012년 ILSVRC에서 AlexNet은 기존 다른 어떤 방식보다 Image Classification 분야에서 압도적인 성능을 보였으며, computer vision 분야에서 다시 CNN 연구를 촉진시켰다. <br /> \n",
    "AlexNet의 주요한 특징으로는 GPU 사용, ReLU를 활성함수로 사용, Drop out 사용, 효과적인 Data augmentation 기법 적용 등, 최근 CNN/DNN 분야에서 <br /> \n",
    "반드시 들어가는 개념이 모두 적용 되었다. <br />\n",
    "Drop out이 처음 발표가 될 당시만 하더라도 FC 레이어에만 적용한 것으로 발표가 되었지만, 이후 2012년, 2013년에 Drop out을 개선한 논문들이 <br /> \n",
    "다른 연구팀에 의해 지속적으로 발표가 되었다. <br /> \n",
    "2014년에 초기 Drop out 개념을 좀 더 이론적으로 정립하고, Conv 레이어에도 적용을 한 논문 “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” <br /> \n",
    "이라는 논문을 발표한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out이란 네트워크의 일부를 생략하는 것이다. 아래의 그림처럼 네트워크의 일부를 생략하고 학습을 진행하게 되면, 생략한 네트워크는 학습에 영향을 끼치지 않게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 결합(model combination)을 하게 되면 학습의 성능을 개선할 수 있다 모델 결합이 효과를 얻으려면, 서로 다른 학습 데이터를 이용해서 학습을 하거나, <br /> \n",
    "모델이 서로 다른 구조를 가져야 한다. <br /> \n",
    "하지만, 네트워크가 깊은 경우에 1개의 네트워크를 학습시키는 것도 만만치 않은 일인데 여러 개의 네트워크를 학습시키는 것은 매우 힘든 작업이 된다. <br />\n",
    "또한 다양한 모델을 학습시켰을지라도 다양한 모델을 실행시킬 때 연산 시간을 잡아먹기 때문에 빠른 response time이 요구되는 경우에는 곤란함이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out은 위 2가지 문제를 해결하기 위해 개발된 것이다. <br /> \n",
    "여러 개의 모델을 만드는 대신에 모델 결합에 의한 투표 효과(Voting)와 비슷한 효과를 내기 위해 학습 사이클이 진행되는 동안 무작위로 일부 뉴런을 생략한다. <br /> \n",
    "그렇게 되면 생략되는 뉴런의 조합만큼 지수 함수적으로 다양한 모델을 학습시키는 것이나 마찬가지이기 때문에 모델 결합의 효과를 누릴 수 있다. <br />\n",
    "또한 실제로 실행을 시킬 때는 생략된 많은 모델을 따로 실행시키는 것이 아니라, 생략된 모델들이 모두 파라미터를 공유하고 있기 때문에 모두 각각의 뉴런들이 존속할 <br />\n",
    "(drop out 하지 않을) 확률을 각각의 파라미터에 곱해주는 형태가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것을 그림으로 표현하면 아래와 같은 형태가 된다. <br /> \n",
    "학습 시에는 뉴런은 존속할 확률 $p$로 학습을 진행하고, 실행할 때는 각각의 네트워크에 얻어진 파라미터에 존속할 확률 $p$를 곱해준다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out을 하면 왜 regularization 효과를 갖는 것일까? <br /> \n",
    "학습을 시키다 보면, 학습 데이터에 의해 각각의 네트워크의 파라미터들이 서로 동조화 되는 현상(co-adaptation)이 발생할 수 있는데 <br />\n",
    "(이것은 뉴럴 네트워크 구조의 본질적 이슈), 무작위로 생략을 하면서 학습을 시킴으로써 이런 동조화 현상을 피할 수 있게 된다. <br />\n",
    "그리고 이렇게 co-adaptation을 피해서 학습을 하게 되면, 아래 그림에서 볼 수 있는 것처럼 좀 더 선명한 feature를 얻게 된다. <br /> \n",
    "아래 그림은 논문의 저자들이 Drop out의 효과를 확인하기 위해 feature visualization을 통해 확인한 결과이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 drop out을 하게 되면, hidden 뉴런들의 활성도(activity)가 좀 더 드문 드문(sparsity) 해지는 경향이 생긴다. <br />\n",
    "아래의 그림에서 왼쪽 그림은 drop out이 없는 일반 뉴럴 네트워크에서 hidden 레이어의 활성도를 보여주고 있는데, 히스토그램이 좀 더 넓게 펴져있는 것을 확인 할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "의도하지 않았지만 좋은 성질을 얻게 되었다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out 모델링은 Drop out이 적용되지 않은 표준 네트워크와 적용된 네트워크를 구별해서 생각하면 좋다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 왼쪽에 있는 표준 네트워크에 Drop out을 적용하면 오른쪽 그럼처럼 모델링 할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 표준 네트워크를 수직으로 표현하면 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $f(x)$는 뉴런의 활성화 함수이다. <br />\n",
    "입력으로 ${ y }_{ i }^{ (l) }$를 받아 네트워크의 가중치 $W_{ i }^{ (l+1) }$를 곱하고 출력으로 $y_{ i }^{ (l+1) }$이 나온다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out을 적용한다는 것은 베르누이 랜덤 변수인 $r_{ i }^{ (l) }$을 곱해주는 것으로 생각할 수 있으며, 그렇게 되면 관련 수식은 아래와 같이 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $*$는 각 항목별로 곱을 해주는 것을 의미한다. <br />\n",
    "입력 ${ y }_{ i }^{ (l) }$에 Drop out 랜덤 변수 $r_{ i }^{ (l) }$을 곱해주면 결과적으로 $r_{ i }^{ (l) }$의 값에 따라 네트워크가 줄어드는 thinned 네트워크 ${ \\overset { \\sim  }{ y }  }_{ i }^{ (l) }$이 되며 <br />\n",
    "여기에 가중치 $W_{ i }^{ (l+1) }$을 곱해준다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 베르누이 랜덤 변수 $r_{ i }^{ (l) }$는 유닛의 존재 유/무 두 가지 값을 갖는 랜덤 변수를 말하며, 유닛이 존재할 확률이 $p$라고 하면, 평균은 $p$이고 분산은 $p(1-p)$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각의 유닛에 대해 독립적으로(independent) 랜덤 변수를 곱해주기 때문에 n개의 유닛에 대해서는 ${2}^{n}$개의 조합이 가능하게 된다. <br />\n",
    "결과적으로 이것은 실제 테스트 시에는 각각의 가중치가 $w_{ test }^{ (l) }=p{W}^{(l)}$이 됨을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-norm Regularization과 같이 사용하면 성능이 더욱 향상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum, annealed learning rates 및 $L1/L2$ weight decay와 같은 기존 regularization을 Drop out과 같이 사용하면 대부분 좋은 결과가 나온다. <br />\n",
    "하지만 이들 중 특히 더 좋은 regularization 방법이 있는데 그것은 max norm이다. <br />\n",
    "Max norm 은 hidden 레이어로 들어오는 weight들이 특정 상수 $c$보다 작게(${ ||W|| }_{ 2 }\\le c$)해주는 방법이다. <br />\n",
    "이 때 상수 c는 조율이 가능한 하이퍼 파라미터이며, validation 데이터 집합을 이용해 결정한다. <br />\n",
    "그리고 max norm 방법은 Drop out을 적용하지 않더라도 좋은 regularizer임이 이미 증명이 되었다. <br />\n",
    "Max norm과 Drop out을 함께 사용 하면, Learning rate를 큰 값을 사용하는 것이 가능하며, 이것은 학습을 빠른 속도로 진행할 수 있음을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out 성능 험을 위한 실험 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out이 과연 효과가 있는지 실험을 위하여 다양한 테스트 데이터 및 구조에 대한 실험을 진행하였으며, 모든 경우에 Drop out을 적용한 결과가 더 좋게 나왔다. <br />\n",
    "다음은 성능 평가에 사용한 실험 데이터들이다. <br />\n",
    "\n",
    "- MNIST: 필기체 숫자 인식용 표준 데이터\n",
    "\n",
    "- TIMIT:  잡음이 없는 상태에서 음성 인식을 위한 표준 데이터\n",
    "\n",
    "- CIFAR-10, CIFAR-100: Krizhevsky 등이 만든 작은 크기의 자연 이미지 집합\n",
    "\n",
    "- Street View House Numbers data set(SVHN): 구글의 Street View에 찍힌 집 주소 이미지.\n",
    "\n",
    "- ImageNet: 대용량 자연 이미지 데이터 베이스\n",
    "\n",
    "- Reuters-RCV1: 로이터 뉴스 기사\n",
    "\n",
    "- Alternative Splicing data set: RNA splicing을 예측하기 위한 RNA feature들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터들에 대한 자세한 정보는 다음과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=700 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 데이터에 대한 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST는 28x28 픽셀 크기의 필기체 숫자를 인식 성능을 평가하기 위한 데이터 집합이며, 아래처럼 기존 표준 뉴럴 네트워크로 구현했을 때와 Drop out을 다양한 방법으로 <br /> \n",
    "적용했을 때를 비교한 것이다. <br /> \n",
    "여기서 DBM은 Deep Boltzmann Machine을 의미한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out이 학습을 진행함에 따라 어떤 결과가 나타나는지 확인하기 위해 같은 하이퍼 파라미터에 $p$를 고정시키고 실험을 하였으며, 그 결과는 Drop out을 <br /> \n",
    "적용시키면 훨씬 성능이 좋고, 학습을 계속 진행시켰을 경우에 꾸준히 성능이 개선되는 것을 확인 할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글 Street View에 찍힌 집 주소: 아래의 그림과 같은 32x32의 작은 크기의 이미지이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10이나 CIFAR-100의 경우도 카테고리의 숫자만 차이가 있을 뿐 모두 32x32의 컬러 이미지를 사용한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVHN 데이터에 average pooling을 적용한 Multi stage Conv Net으로 실험을 하였을때는 9.06%의 error rates를 보였으며, <br />\n",
    "average pooling 대신에 max pooling을 적용한 경우에는 3.95%로 error rates가 낮아졌다. <br />\n",
    "여기에 FC 레이어에 대해서만 Drop out을 적용하면 3.02%였고, 모든 레이어에 Drop out을 적용하면 error rates가 2.55%로 아주 훌륭한 결과가 나왔다. <br />\n",
    "결과는 아래 표와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/14.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10과 CIFAR-100의 실험 결과 역시 FC 레이어에만 drop out을 적용해도 성능 개선이 확인되었으며 모든 레이어에 Drop out을 적용하면 훨씬 성능이 <br /> \n",
    "개선되는 점을 확인할 수 있었으며, 결과는 아래 표와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/15.png\" width=700 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 음성 데이터(TIMIT)에 대한 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIMIT 데이터는 미국식 영어의 대표적인 8개 방언을 사용하는 680명을 대상으로 잡음이 없는 환경에서 녹음을 한 것이다.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험 결과는 6-레이어의 뉴럴 네트워크를 이용한 것과 DBN(Deep Belief Network) 경우로 나누어서 진행을 했는데, 위의 표처럼 어느 경우에나 Drop out을 사용하면 결과가 좋아졌다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로이터의 80만개 이상의 기사를 대상으로 50개의 topic으로 분류하는 문서 분류기를 뉴럴 네트워크로 개발을 하였는데 에러율이 31.05%가 나왔다. <br />\n",
    "여기에 Drop out을 적용하여 다시 학습을 하였더니 에러율이 29.62%로 떨어졌으며, 이를 통해 text data에 대해서도 Drop out이 효과적임을 알 수 있다. <br />\n",
    "또한 RNA feature들을 기반으로 alternative splicing이 일어나는 것을 예측하는 경우에도 Drop out을 사용하는 것이 효과적임이 밝혀졌다. <br />\n",
    "위와 같이 다양한 데이터 및 다양한 구조에 Drop out을 적용하였을 때 모두 성능이 개선 되는 것을 확인할 수 있었으며, Drop out은 확실하게 뉴럴 네트워크의 <br /> \n",
    "성능을 개선시킨다는 것이 입증되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 표는 MNIST 데이터에 대한 실험 결과이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/17.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop out의 부가 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out을 사용하면 사용하지 않을 때보다 좀 더 선명한 특징(salient feature)을 끌어낼 수가 있게 된다. <br />\n",
    "그 이유는 뉴런들을 무작위로 생략시키면서 학습을 시키면 파라미터들이 서로 동화(co-adaptation) 되는 것을 막을 수 있어, 좀 더 의미 있는 특징들을 추출하는 것으로 해석된다. <br /> \n",
    "즉, 다른 파라미터와 같이 cost(loss) function을 줄여나가다 보면 파라미터의 공조 현상이 일어날 수 있는데, Drop out을 하게 되면 서로 의지하던 것을 <br /> \n",
    "스스로 해줘야 하기 때문에 좀 더 의미 있는 feature를 끄집어낼 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out을 통해 얻을 수 있는 또 다른 부가 효과는 hidden 레이어에 있는 큰 activation을 보이는 뉴런의 수가 줄어들게(sparse) 된다. <br /> \n",
    "Drop out을 시키고 hidden 레이어에 대한 뉴런들의 activation에 대한 히스토그램을 구하면, 0에서 큰 peak가 1개 있고, activation이 큰 값을 보이는 뉴런은 몇 개 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter $p$(Drop out 확률 변수)가 성능에 미치는 영향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out에서 중요한 하이퍼 파라미터 변수인 $p$(엄밀한 의미에서는 존속할 확률 변수)를 변화시키는 실험을 다음 두 가지 경우에 대하여 수행하였으며, <br /> 이를 통해 $p$의 변화가 결과에 어떤 영향을 주는지 확인하였다. <br />\n",
    "\n",
    "- Hidden 레이어에 있는 뉴런의 개수를 고정시키고 $p$만 변화 시켜가면서 테스트\n",
    "- $p$값을 변화시키면서 hidden 레이어에 있는 뉴런의 수를 같이 변화시키면서 테스트 (n을 hidden layer에 있는 뉴런의 개수라고 한다면 np가 일정하게)\n",
    "\n",
    "위 두가지 상황에 대한 실험 결과는 아래 그림과 같다. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/18.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden 레이어에 있는 뉴런의 수를 고정하고 $p$를 변화시키면서 실험을 하면, 0.4< $p$ < 0.8 범위에서는 $p$ 값이 변하더라도 검사 오차가 거의 일정하게 나오는 것이 확인이 되었다. <br /> \n",
    "0.4 이하에서는 Drop out이 많아 underfitting이 일어나는 것으로 추정이 되며, 0.8이상이면 Drop out이 적기 때문에 overfitting의 효과가 나타나는 것으로 해석된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림의 오른쪽은 $np$를 일정하게 유지하는 경우에 대한 실험인데 $p$가 낮을 때는 n만 고정시켰을 때보다 오차율이 낮은 것을 확인할 수 있었다. <br /> \n",
    "이는 Drop out을 많이 시키고자 한다면 뉴런의 개수를 키우는 것이 underfitting의 영향을 줄일 수 있다는 것을 확인하였으며, p가 0.6일 때 가장 결과가 좋을 것을 알 수 있었다. <br /> \n",
    "0.6과 0.5는 거의 유사한 값이기 때문에 계산의 편의를 위해 hidden 레이어에서의 Drop out 확률 $p$를 0.5로 정하는 것은 효과적이라는 것도 입증한다고 볼 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 데이터 양에 따른 Dropout 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 양에 따른 Drop out 결과를 확인하기 위해 MNIST 데이터 중에서 무작위로 100, 500, 1K, 5K, 10K 및 50K를 골라 네트워크 구조는 동일한 조건에서 <br />\n",
    "실험을 하였으며, 결과는 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/19.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터의 양이 극히 적은 (100, 500)의 경우는 Drop out을 적용한 경우에 오히려 결과가 나쁘게 되었지만, 그 외의 경우는 모두 Drop out이 효과적이라는 것을 확인할 수 있다. <br /> \n",
    "데이터의 양을 계속 늘리면 어느 순간을 지나면 Drop out의 효과가 줄어드는 것을 확인할 수 있는데, 이는 데이터의 양이 많아지면서 과적합 가능성이 줄어들었기 때문으로 해석된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DropConnect의 기본 개념 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out이 발표되고, 효과적인 방식이라는 것이 받아들여지면서, Drop out 논문에 자극을 받아 Drop out을 변형, 개선 및 수학적으로 증명하는 많은 논문들이 발표되었다. <br />\n",
    "그 중 주목을 받은 방식 중 하나가 Drop Connect 이다. <br />\n",
    "Drop Connect는 2013년 뉴욕대 팀이 발표를 하였으며, 논문 “Regularization of Neural Networks using DropConnect – Li Wan, Matthew Zeiler, Yann LeCun, <br /> \n",
    "Lob Fergus”를 참고 하면 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/20.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out과 Drop Connect의 차이는 위 그림에서 볼 수 있는 것처럼, Drop out은 뉴런을 생략하는 것이고, Drop Connect는 connection을 생략(결과적으로 <br />\n",
    "weight를 생략하는 것이며 뉴런(노드)은 남아 있음)하는 것이다. <br /> \n",
    "Drop out과 Drop Connect에 대하여 수식으로 차이점을 명쾌하게 설명한 논문 “The Dropout Learning Algorithm – Pierre Baldi & Peter Sadowski”을 참고하여라.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k개의 출력을 갖는 표준 뉴럴 네트워크의 출력은 다음과 같은 식으로 표시가 가능하다. 아래 식에서 ${w}_{ij}$는 각 connection의 가중치(weight)에 해당된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/21.png\" width=400 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 위 표준 뉴럴 네트워크에 Drop out을 적용하여 입력 노드(뉴런)를 생략하게 되면, 출력 변수는 아래와 같이 표현이 가능하다. <br /> \n",
    "아래 수식에서 ${\\delta}_{j}$는 노드의 생략에 관련된 베르누이 랜덤 변수이며, 이것은 ${p}_{j}$로 표시가 가능하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/22.png\" width=400 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 동일 뉴럴 네트워크에 Drop Connect가 적용이 된다면, 출력 변수는 아래와 같이 표현이 가능하다. <br /> \n",
    "위와 달라진 점이 있다면, 노드를 생략하는 경우는 노드 생략에 관련된 베르누이 랜덤 변수 ${\\delta}_{j}$ 대신에 connection(net)을 생략하기 위한 ${\\delta}_{ij}$ 가 붙었다는 점이다. <br />\n",
    "이 점을 제외하면 거의 동일하다는 것을 알 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop connect의 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop out과 Drop Connect는 생략을 통해 파라미터가 동조(co-adaptation)하는 것을 막는다는 관점에서는 비슷하지만, Drop Connect가 Drop out을 더 일반화 시킨 것이다. <br /> \n",
    "Drop out처럼 노드(뉴런)를 생략하면 관련된 connection(weight)가 모두 생략이 되는 것이지만, connection만 생략하게 되면 훨씬 가능한 모델이 많이 나올 수 있다. <br />\n",
    "앞서 살펴본 수식도 이런 연유로 그렇게 된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Connect 논문 저자들은 Drop Connect 방식의 성능 분석을 위해 사용한 방식은 Drop out 논문 저자들이 했던 것과 동일한 벤치마크 데이터를 이용하는 것이다. <br /> \n",
    "직접적인 비교를 하게 되면 바로 성능 비교가 가능하기 때문이다. <br /> \n",
    "하지만 기대했던 것처럼, 눈에 띌 수준은 아니지만 Drop Connect 방식이 Drop out 방식보다 조금 더 효과가 있는 것으로 나타났다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 MNIST 데이터를 이용해 실험을 하였는데 결과는 다음 표와 같다. <br /> \n",
    "사용한 함수에 따라 Drop Connect 방식과 Drop out 방식의 성능이 엇갈린 결과가 나왔다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/24.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10에 대한 실험 결과는 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/26.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 활성화 함수로 ReLU를 사용하였으며, Drop out이나 Drop Connect 모두 사용하지 않을 때보다는 결과가 좋지만, 두 방식의 성능차를 확신할 수준은 아니다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVHN 데이터를 이용한 실험 결과는 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/28.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 실험 결과를 보면 Drop out이나 Drop Connect를 적용한 결과가 적용하지 않을 때와 비교하여 그리 좋다고 볼 수 없지만, 이는 논문 저자들의 학습 방법이 <br /> \n",
    "SVHN 데이터를 효과적으로 학습시키기 위해 적용한 방식이 탁월했기 때문이라 해석된다. <br /> \n",
    "다른 논문에 나온 실험 결과가 거의 2.8% 수준이고 DropOut 논문에서 2.4%로 밝혔는데 여기서는 model voting을 통해 error rates를 모두 1%대로 개선을 시켰다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이론적으로는 Drop Connect의 자유도(표현력)가 높아 Drop out보다 좋아야 할 것 같은데, 약간 좋은 수준이라서 논란의 여지는 있다. <br />\n",
    "Drop Connect 논문이나 Drop out 논문에서 매우 깊은 네트워크에 대한 실험을 한 것은 아니지만, Drop out이나 Drop Connect가 효과적이라는 것은 <br /> \n",
    "이미 여러 곳에서 입증이 되었다. <br /> \n",
    "그 만큼 뉴럴 네트워크 학습에서 과적합 문제는 크고, 이 문제 해결을 위한 적절한 regularization 방법의 선택이 중요하며, Drop out 혹은 Drop Connect 는 아주 효과적인 기법이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
