{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순환 신경망(Recurrent Neural Networks)\n",
    "순환 신경망은 그 내부에 루프를 가진 네트워크입니다. 그 루프는 정보가 지속되는 것을 돕습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/1.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 다이어그램에서, 신경망 A는 입력 x(t)를 보고 값 h(t)를 출력합니다. <br />\n",
    "루프는 정보가 네트워크의 한 단계에서 다음 단계로 전달되도록 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/2.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 루프들은 보통 신경망들과 크게 다르지 않습니다. <br />\n",
    "순환 신경망은 동일한 네트워크를 여러 개 복사한 것으로 생각될 수 있습니다. <br />\n",
    "각 네트워크는 자신의 후임자에게 메시지를 전달합니다. <br />\n",
    "위 다이어그램은 순환 신경망의 루프를 펼친 것 입니다. <br />\n",
    "이 사슬같은 특성은 순환 신경망이 시퀀스 그리고 리스트와 밀접하게 연관되었음을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 장기 의존성 문제(The Problem of Long-Term Dependencies)\n",
    "순환 신경망의 매력 중 하나는 이전의 정보를 현재의 작업으로 연결된다는 것입니다. <br />\n",
    "예를 들어, 이전 단어들의 정보를 사용하여 다음 단어를 예측하는 언어 모델을 생각해보겠습니다. <br />\n",
    "'The clouds are in the'라는 문장에서 문맥을 고려한다면 마지막 단어로 sky가 나오는 것은 꽤 분명합니다. <br />\n",
    "적절한 정보와 그 정보가 필요한 곳의 거리가 가까운 경우, 순환 신경망은 과거 정보를 사용하여 학습할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/3.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 문맥이 더 필요한 경우도 있습니다. <br />\n",
    "'나는 프랑스에서 자랐습니다.' '나는 유창한 프랑스어를 합니다.'라는 문장에서 프랑스라는 단어를 예측하려고 생각해봅시다. <br />\n",
    "최근 정보는 아마도 다음 단어가 어느 나라 말일 지를 암시할 것입니다. <br />\n",
    "그러나 만약에 그것이 어떤 언어인지까지 좁히기를 원하면, 더 앞쪽에 있는 프랑스라는 문맥이 필요합니다. <br />\n",
    "이는 관련 정보와 그 정보를 사용하는 지점 사이의 거리가 매우 멀어질 수 있을 때만 가능합니다. <br />\n",
    "불행히도, 그 거리가 멀어질수록 일반적인 순환 신경망은 그 정보의 연결 방법을 배울 수 없게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 LSTM에는 이런 문제가 없습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 네트워크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM은 장기 의존성을 학습할 수 있는 특별한 종류의 순환 신경망입니다. <br />\n",
    "LSTM에서는 오랫동안 정보를 기억하는 것이 기본 동작입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/5.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 순환 신경망은 사슬 형태의 반복되는 신경망 모듈들을 가집니다. 표준 순환 신경망에서, <br />\n",
    "이 반복되는 모듈은 한 개의 tanh층 과 같은 매우 간단한 구조를 가질 것 입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/6.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 또한 같은 사슬 구조를 가집니다. 그러나 반복되는 모듈은 다른 구조를 가집니다. <br />\n",
    "이 모듈에는 하나의 신경망 층 대신 매우 특별한 방식으로 상호작용하는 네 개의 층이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/7.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노란색 상자는 학습된 신경망 층입니다. <br />\n",
    "분홍색 원은 벡터 덧셈 같은 요소별 연산을 나타냅니다. <br />\n",
    "각 화살표는 한 노드 출력에서 다른 노드의 입력으로 전체 벡터 하나를 전달합니다. <br />\n",
    "합쳐지는 화살표들은 연관(concatenate)를 표시합니다. <br />\n",
    "갈라지는 화살표는 그 내용이 복사되어 다른 곳으로 보내짐을 표시합니다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM의 핵심 아이디어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/8.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM의 핵심은 셀 상태(Cell State) 즉, 다이어그램의 위쪽을 통과해 지나는 수평선입니다. <br />\n",
    "셀 상태는 일종의 컨베이어 벨트입니다. 셀 상태는 약간의 가벼운 선형 상호작용만 일으키며 전체 수평선을 그냥 똑바로 지나갑니다.<br />\n",
    "대부분 정보는 바뀌지 않은 채 그냥 흘러가기 쉽습니다. 장기 정보를 쭉 가져가는 역할을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM은 셀 상태에 정보를 더하거나 지울 수 있습니다. 게이트라 불리는 구조들이 이 과정을 신중히 조정합니다. <br />\n",
    "게이트는 정보가 선택적으로 지나가게 합니다. 게이트는 시그모이드 신경망 층과 요소별 곱셈 연산으로 구성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/9.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 층은 0에서 1사이의 숫자를 출력합니다. 이 출력 값은 얼마나 많은 요소가 통과되어야 하는지를 나타냅니다. <br />\n",
    "예를들어 0은 아무것도 통과 못함, 1은 모두 통과함을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM을 단계별로 차례차례 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/10.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM에서 첫 단계는 셀 상태에서 어떤 정보를 버릴지 결정하는 것입니다. <br />\n",
    "이는 'Forget 게이트 층'이라 불리는 한 시그모이드 층에 의해 결정됩니다. <br />\n",
    "이 층은 h(t-1)과 x(t)를 보고 f(t)값을 출력합니다. (0과 1 사이 숫자를) <br />\n",
    "셀 상태 c(t-1)에 대하여 f(t)값이 1 인 경우 '이것을 완전히 유지함', 0은 '이것을 완전히 제거함'을 나타냅니다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/11.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단계에서는 어떤 새로운 정보를 셀 상태에 저장할지 결정합니다. <br />\n",
    "이 단계는 두 부분으로 구성됩니다. <br>\n",
    "첫째: Input 게이트 층이라고 불리는 한 시그모이드 층은 우리가 어떤 값들을 갱신해야 할지 결정합니다. <br />\n",
    "다음 tanh 층은 셀 상태에서 더해질 수 있는 새로운 후보값들의 백터 C(t)를 만듭니다. <br />\n",
    "다음 단계에서, 셀 상태를 갱신할 값을 만들기 위해 이 둘을 합칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/12.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 단계에서는 이전 셀 상태 C(t-1)을 C(t)로 갱신합니다. <br />\n",
    "이전 단계에서 셀 상태를 어떻게 할지 결정하였으므로 이 단계에서는 그 결정된 일을 수행하면 됩니다. <br />\n",
    "이전 상태의 C(t-1)에 f(t)를 곱합니다. f(t)는 이전에 계산한 Forget 게이트의 출력입니다. <br />\n",
    "f(t)는 우리가 잊기로 결정한 것들을 잊게 만드는 역할을 합니다. 그런 다음 i(t)*C(t)를 더합니다. <br />\n",
    "이것이 각 상태 값을 얼만큼 갱신할지 결정한 값입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/13.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 무엇을 출력할지 결정합니다. <br />\n",
    "이 출력은 셀 상태에 기반을 두지만 여과된 버전입니다. <br />\n",
    "우선 시그모이드 층을 동작시킵니다. 이 시그모이드 층은 셀 상태에서 어떤 부분들을 출력할지 결정합니다. <br />\n",
    "그런 다음 값이 -1 과 1 사이 값을 갖도록 셀 상태를 tanh에 넣습니다. <br />\n",
    "그리고 오직 우리가 결정한 부분만 출력하도록, tanh 출력을 다시 시그모이드 게이트 출력과 곱합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM의 변형들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peep hole 연결하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 연결은 각 게이트 층들이 셀 상태를 자세히 볼 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/14.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 다이어그램을 살펴보면 Peep hole들을 모든 게이트에 추가합니다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forget 게이트와 Input 게이트 간의 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 것을 잊을지 그리고 어떤 새로운 정보를 더할지를 따로 결정하지 않고 한번에 결정합니다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/15.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오직 무언가를 입력하려고 할 때만 잊습니다. 더 오래된 어떤 것을 잊으려고 할 때만 새 값들을 상태에 입력합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (더 자세히 연구하기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU는 Forget 게이트와 Input 게이트를 하나의 단일 Update 게이트로 합칩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RNN_1_images/16.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "또한 셀 상태와 Hidden State를 합칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기타\n",
    "Clockwork RNN <br />\n",
    "Attention (주목)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
