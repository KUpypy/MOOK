{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번역자: 고려대학교 수학과(12) 최시현\n",
    "[AiKorea 번역 프로젝트](https://github.com/aikorea/cs231n)를 이어받아서 진행하였습니다. 기존 AiKorea에서 이미 번역되어있는 CourseNote의 경우 유사한 부분이 많습니다. <br /> \n",
    "(깨진 삽화를 추가하거나 개인적으로 번역이 매끄럽지 못하다 생각하는 부분은 수정하였습니다.) <br />\n",
    "문제가 될시에는 자진 삭제하겠습니다. 오류 수정 및 내용에 대한 기타 문의 사항은 passkmla@naver.com으로 연락주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ConvNet이 학습한 것을 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴 네트워크에서 학습된 특징들은 해석하기 힘들다는 비판에 대한 대응으로, <br /> \n",
    "ConvNet을 이해하고 시각화하기 위한 몇가지 접근법들이 개발되고 있다. <br />\n",
    "이 장에서 이것과 간련된 접근법들에 대해 간단히 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the activations and first-layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 직접적인 시각화 기법은 forward pass동안 네트워크의 activations를 보여주는 것이다. <br />\n",
    "ReLU 망의 경우, activations는 상대적으로 얼룩지고 밀집되어있는것으로 보이기 시작한다. <br />\n",
    "그러다가 학습 프로세스가 진행됨에 따라 더 희소하고 지역적이게 된다. <br /> \n",
    "이 시각화에서 쉽게 알아차릴수 있는 하나의 pitfall은 몇몇 activation 맵에서 많은 서로 다른 입력들에 대하여 0의 값을 갖는다는 것이다. <br />\n",
    "이는 죽은 필터를 나타내는 것이고, 높은 학습률의 징조일 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 Conv 레이어의 typical-looking activations(왼쪽)와, 고양이 그림을 보고있는 학습된 AlexNet의 5번째 Conv 레이어 <br /> \n",
    "모든 상자는 어떤 필터에 상응하는 activation 맵을 보여준다. <br /> \n",
    "activations는 희소(대부분의 값이 0, 검은색으로 시각화 됨)이고 대부분 지역적이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv/FC 필터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째로 보통 사용되는 전략은 가중치들을 시각화하는 것이다. 원본 필섹 자료에서 직접보는 첫 번째 Conv 레이어는 대게 해석하능하지만, <br />\n",
    "네트워크에서 더 깊어진 필터의 가중치들을 보는 것 또한 가능하다. 이 가중치들은 시각화 하기에 유용하다. <br /> \n",
    "왜냐하면, 잘 학습된 네트워크는 대게 잘 display하고 어떤 노이즈 패턴 없이 smooth한 필터를 보여주기 때문이다. <br />\n",
    "노이즈 패턴은 충분히 학습되지 않은 망의 지표가 될 수있고, <br /> \n",
    "과적합의 원인이 되는 너무 낮은 정규화(regularization) 강도를 나타내는 지표일 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 AlexNet에서 첫 번째 Conv 레이어(왼쪽)와 2번째 Conv 레이어(오른쪽)에서 일반적으로 보이는 필터. <br />\n",
    "첫 번째 레이어 가중치들은 매우 좋고 부드럽다. 잘 수렴된 네트워크를 나타낸다. <br /> \n",
    "AlexNet은 processing의 두 가지 나누어진 흐름을 포함하고 있기 때문에 색 / gray-scale features들은 군집된다.\n",
    "하나의 흐름은 high-frequency gray-scale을 발달시키고, 다른 하나는 low-frequency color feature를 발달시킨다는 것이다. <br />\n",
    "이 두 흐름은 이 아키텍처의 결과이다. <br />\n",
    "두 번째 Conv 레이어의 가중치들은 해석가능하지 않지만, 이것이 여전히 부드럽고, 잘 형성되어 있고, 노이즈 패턴이 없는 것은 명백하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최대로 뉴런을 활성화시키는 이미지를 되찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또다른 시각화 기법은 많은 이지미들을 가지고, 이것들을 망을 통해보내서 뉴런을 가장 잘 활성화시키는 이미지를 추적하는 것이다. <br />\n",
    "우리는 리셉티브 필드에서 뉴런이 찾고 있는 것이 무엇인지 이해하기 위해 이미지를 시각화 할 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet의 5번째 풀링에이어를 가장 잘 활성화시키는 이미지. <br />\n",
    "특정 뉴런의 리셉티브 필드와 활성화 값은 흰색으로 보인다. (특히, 5번째 풀링 레이어의 뉴런들은 상대적으로 입력 이미지의 큰 부분이다) <br />\n",
    "이것은 어떤 뉴런이 상체나 특정 하이라이트에 반응하는것 처럼 보일 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 접근의 한가지 문제는 ReLU 뉴런은 스스로에 의해 어떤 의미론적인 의미가 필연적으로 필요한 것은 아니라는 점이다. <br />\n",
    "오히려, 이것은 이미지 patches에 어떤 공간을 나타내는 기저 벡터들로서 ReLU 뉴런들에 대해 생각하는 것이 적절하다. <br />\n",
    "즉, 이 시각화 필터는 임의의 필터 가중치에 해당하는 축을 따라 모서리 구름들의 표현이 대표되는 patches를 보여준다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE와 함께 코드 embeding하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet은 점차적으로 어떤 이미지를 선형 분류기에 의해 분리가능한 클래스에 대한 표현으로 변화시키는 것으로 해석할 수 있다. <br />\n",
    "이미지를 2차원으로 embeding하는 공간의 위상에 대한 대략적인 아이디어를 가짐으로서 이미지들의 저차원 표현이 고차원 표현에 대략적으로 동일한 거리의 근사를 가진다. <br />\n",
    "쌍으로 이루어진 점들의 거리를 유지시키면서 고차원을 저차원으로 embeding하는 intuition을 발전시키는 많은 embeding 방법들이 있다. <br />\n",
    "이들 중에 [t-SNE](http://lvdmaaten.github.io/tsne/)는 시각적으로 만족스러운 결과를 일관되게 보여주는 가장 잘 알려진 방법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeding을 만들기 위해, 이미지들의 집합을 가지고 CNN 코드를 추출하기위해 ConvNet을 사용한다. <br />\n",
    "(e.g. AlexNet에서 ReLU의 비선형성을 포함하는 분류기 바로 직전의 4096 차원의 벡터) <br />\n",
    "이것들을 t-SNE에 연결하여 각각의 이미지에 대해 2차원 벡터를 얻을 수 있다. <br />\n",
    "해당 이미지들은 격자에서 시각화 될 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 코드에 기반한 t-SNE Embeding 이미지들의 집합 <br />\n",
    "CNN 표현 공간에서 서로 가까이 있는 이미지들은 그들을 유사하게 보고있음을 의미한다. <br />\n",
    "이 유사성은 픽셀과 색 기반 보다는 더 클래스 기반이고 의미론적이다. <br />\n",
    "For more details on how this visualization was produced the associated code, and more related visualizations at different scales refer to [t-SNE visualization of CNN codes.](http://cs.stanford.edu/people/karpathy/cnnembed/)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지의 가려진 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet이 이미지를 개로 분류한다고 가정하자. <br />\n",
    "어떻게 실제로 이미지에서 여러 다른 종류의 객체나 배경으로부터 나온 몇몇 맥락적 단서에 반대하여 개를 선택할 것을 확신할 수 있을까? <br />\n",
    "이미지 분류기의 예측에서 이미지의 부분을 조사하는 한가지 방법은 <br /> \n",
    "가려진 객체의 위치의 함수로서 관심있는 클래스(e.g. dog class)의 확률을 나타내는 것이다. <br />\n",
    "즉 이미지의 영역에 대하여 반복하는데, 이미지의 patch는 전부 0로 설정하고, 클래스의 확률을 본다. <br />\n",
    "2차원 히트 맵으로 확률을 시각화 할 수 있다. <br />\n",
    "This approach has been used in Matthew Zeiler’s [Visualizing and Understanding Convolutional Networks:](https://arxiv.org/abs/1311.2901)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세개의 입력 이미지(위) <br />\n",
    "가려진 부분은 회색으로 표시되었다. 가려진 부분을 이미지에 대해 움직이면서 올바른 클래스일 확률을 기록하고 이것을 히트맵으로 시각화한다. <br />\n",
    "(각 이미지 아래에 보여지는 것처럼)< br /> \n",
    "예를 들어, 가장 왼쪽 이미지에서 포메라니안 일 확률은 가려진 부분이 개의 얼굴일 때 급락하는 것을 볼 수 있고, <br />\n",
    "따라서 개의 얼굴이 높은 분류 스코어를 위한 주요 원인임을 확신하게 해준다. <br />\n",
    "이미지의 다른 부분을 0으로 만드는 것은 상대적으로 무시할만한 영향을 가진다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data gradient and friends\n",
    "\n",
    "#### Data Gradient.\n",
    "\n",
    "[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)\n",
    "\n",
    "#### DeconvNet.\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "#### Guided Backpropagation.\n",
    "\n",
    "[Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)\n",
    "\n",
    "#### Reconstructing original images based on CNN Codes\n",
    "\n",
    "[Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035)\n",
    "\n",
    "#### How much spatial information is preserved?\n",
    "\n",
    "[Do ConvNets Learn Correspondence?](http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf) (tldr: yes)\n",
    "\n",
    "#### Plotting performance as a function of image attributes\n",
    "\n",
    "[ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)\n",
    "\n",
    "#### Fooling ConvNets\n",
    "\n",
    "[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)\n",
    "\n",
    "#### Comparing ConvNets to Human labelers\n",
    "\n",
    "[What I learned from competing against a ConvNet on ImageNet](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
