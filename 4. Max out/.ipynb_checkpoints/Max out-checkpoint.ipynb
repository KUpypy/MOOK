{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ian J. Goodfellow(Yoshua Bengio의 지도학생)가 발표한 “Maxout Networks” 라는 논문을 중심으로, Max out에 대한 독특한 설명으로 이해를 쉽게 한 <br /> \n",
    "“From Maxout to Channel-Out: Encoding Information on Sparse Pathways” 논문 또한 참고를 하여 “Maxout”에 대하여 살펴보자다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max out을 설명하는 설명하는 수식은 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 수식에서 h는 hidden 레이어를 나타내는 함수이고, 입력으로 들어오는 값들에서 최대값을 취한다. <br />\n",
    "여기서 $W$와 $b$는 학습을 통해서 결정이 되는 파라미터이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 web-site를 참고하라. [http://www.simon-hohberg.de/2015/07/19/maxout.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 그림은 Maxout을 설명하기 위한 그림이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 hidden 레이어는 1개의 layer로 구성이 되는 것과 달리, Maxout hidden 레이어는 2개의 layer로 구성이 되어 있다. <br /> \n",
    "그 구성은 affine function을 수행하는 녹색 영역과 최대값을 선택하는 파란색 영역으로 구성이 된다. <br />\n",
    "녹색 영역은 전통적인 hidden layer 처럼 활성화 함수가 있는 것이 아니라 단순하게 입력 $x$를 각각의 weight에 곱해서 더하는 형식이기 때문에 affine function이라고 부른다. <br /> \n",
    "(녹색 영역에는 non-linear 함수가 없다) <br />\n",
    "위 그림을 보면 k개의 column이 있는데 이 k개의 column에서 동일 위치에 있는 것들 중에서 최고값을 파란색 영역에서 취하며, 최종적으로 m개의 결과가 나오게 된다.\n",
    "___\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max out의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 예제를 살펴보자. <br />\n",
    "입력이 2개이고 출력은 1개뿐이며, k가 3인 max out unit을 그려보면 아래의 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=700 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이 3개의 유닛을 이용하여 $f(x)={x}^{2}$을 근사시킨다고 하며느 아래의 그림과 같은 형태가 나올 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)$를 3개의 직선으로 근사를 시킨다면 위와 같은 모양이 되며, 원래 곡선이 볼록(convex)한 경우는 각각의 직선 중 가장 큰 값을 선택하면, 비교적 비슷한 모양을 얻을 수 있다. <br /> \n",
    "당연히 k값이 클수록 구간이 좀 더 세분화 되면서 원래 곡선과 비슷한 형태를 갖게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 직선으로 근사시키는 상황이기 때문에 오차가 많은 것처럼 보이지만, affine 함수가 갖는 다양한 표현력을 고려하면 k 값이 크지 않더라도 convex 함수를 <br /> \n",
    "거의 표현할 수 있게 된다. <br /> \n",
    "그런 의미에서 Max out은 universal approximator라고 생각할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 Max out은 affine 함수 부분과 최대값을 선택하는 부분을 이용해 임의의 볼록 함수(convex function)를 piecewise linear approximation을 <br />\n",
    "하는 것이라고 할 수 있다. <br /> \n",
    "아래 그림은 다양한 함수를 표현할 수 있는 것을 보여주는 예이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=700 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max out의 성능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxout의 성능 평가를 위해 표준 benchmark 데이터를 이용한 실험을 실시하였다. <br />\n",
    "먼저 MNIST 데이터에 대한 실험 결과는 아래 표와 같다. <br /> \n",
    "앞선 Class에서 살펴본 Stochastic pooling을 써도 성능 개선이 있지만, Maxout을 적용하면 더 좋은 결과를 얻을 수 있음이 밝혀졌다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 데이터에 대한 실험 결과는 아래 표와 같다. <br /> \n",
    "역시 Maxout을 사용하는 경우에 성능이 크게 개선되는 것을 확인할 수 있으며, data augmentation까지 같이 사용 하는 경우에 가장 좋은 결과가 나왔다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-100 데이터에 대한 실험 역시 Maxout을 사용한 경우에 가장 좋은 결과를 얻을 수 있으며, 아래 표와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVHN에 대한 실험 결과 역시 Maxout을 적용한 경우에 가장 좋은 결과가 나왔으며, 아래 표와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max out을 사용하면 왜 이렇게 좋은 결과가 나오는 것일까? <br />\n",
    "그 이유는 Max out에 Drop out을 적용하면서 생기는 model averaging 효과 때문이다. <br />\n",
    "1개의 뉴럴 네트워크에 Drop out을 하면서 학습을 시키면, 마치 엄청나게 많은 수의 sub-model을 학습시키는 것과 유사한 효과를 얻을 수 있고, 이것을 실제 적용할 때는 <br /> \n",
    "각 weight에 drop out 확률 $p$를 곱해주는 간단한 방식으로 model averaging(ensemble 효과)을 얻을 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수학적으로는 Softmax 함수를 사용하는 경우 $p$를 곱해주는 것으로, Softmax layer가 1 layer인 경우에 한하여 $p$를 곱하는 것의 영향이 정확하게 averaging이 <br /> \n",
    "된다는 것만 증명이 되었다. <br /> \n",
    "하지만 Softmax는 최종 단에서 분류 확률을 정할 때 사용할 수 있는 활성화 함수이지 중간 단에 사용할 수 있는 활성화 함수는 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid, tanh 및 ReLU와 같은 함수를 활성화 함수를 사용하여도 Drop out을 적용하면 효과가 있는 것이 확인이 되었다. <br /> \n",
    "이 함수들을 적용할 때도 model averaging을 위해 drop out 확률 $p$를 곱해주는데 이것은 경험적으로 효과가 있다는 것이 밝혀진 것이지 수학적으로 그런 것은 아니다. <br />\n",
    "Maxout 논문의 저자는 Drop out 효과에 깊은 인상을 받아, 그것을 바탕으로 어떻게 하면, Drop out과 결합을 했을 때 가장 좋은 결과를 얻는 활성함수를 만들 수 있을까 고민하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 연속 함수는 적당한 오차 범위 내에서 PWL(Piecewise Linear Approximation)을 이용해 근사 시킬 수 있으며, 오차를 가장 최소화 할 수 있는 방법을 마련하기 위해 <br /> \n",
    "최대값을 선택할 수 있도록 했으며, PWL을 위해 affine function을 구현하다 보니 전통적인 의미에서의 활성화 함수에서 벗어나는 이상한 모양의 네트워크가 나오게 되었다. <br />\n",
    "결과적으로 sigmoid나 tanh와 같은 함수 대신에 PWL 기능을 갖는 활성화 함수를 고안해 냈고, 이를 통해 Drop out 효과를 극대화 시킬 수 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model averaging의 효과를 확인하기 위해, 동일 뉴럴 네트워크에 활성함수만 Maxout과 tanh를 적용하여 실험한 결과는 아래와 같다. <br /> \n",
    "Max out을 사용하는 경우가 tanh를 사용하는 것보다 좋은 결과를 얻을 수 있다. <br />\n",
    "결론적으로 tanh를 사용하는 것보다 Max out을 사용하는 것이 model averaging의 효과를 극대화 시킬 수 있다. <br />\n",
    "또한 ReLU를 사용할 때와 비교하면 training 시에 최적화에 좀 더 유리하다. <br /> \n",
    "위의 결과는 ReLU와 Max out을 사용하여 레이어 수의 증가에 따른 비교 실험 결과이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max out을 사용하면, layer 수가 많아지더라도 완만하게 성능이 나빠지는 것을 확인할 수 있으며, 이것은 overfitting에 있어 ReLU를 사용하는 것보다 <br /> Max out이 훨씬 효과적이라는 의미가 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qi Wang의 논문에 따르면, Max out이 ReLU보다 성능이 좋은 점을 좀 더 다른 관점에서 해석을 하였는데, 이것을 “경로(pathway)”의 관점에서 해석을 하였다. <br /> \n",
    "Max out을 사용하면, 여러 개의 경로 중 하나를 선택할 수 있고, gradient가 back-propagation 되는 방향도 그 경로로만 전파가 되며, 이것은 학습 샘플로부터 얻은 정보를 <br /> \n",
    "sparse한 방식으로 encoding이 가능하다는 뜻이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 ReLU를 사용할 때도 sparse한 성질이 있기는 하지만, 경로 선택에 있어서의 자유는 제한적이며, 이런 이유로 인해서 ReLU를 사용하는 것이 좀 더 과적합될 가능성이 높다고 본다. <br />\n",
    "물론 학습할 때는 sparse한 경로를 통해 encoding하지만, test할 때는 모든 파라미터를 사용하기 때문에 model averaging의 효과를 얻을 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
