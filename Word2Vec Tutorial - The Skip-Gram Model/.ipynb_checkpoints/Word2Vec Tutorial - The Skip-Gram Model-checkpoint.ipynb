{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip gram neural network 모델은 실제로 가장 기본적인 형태로 매우 간단하다. <br />\n",
    "I think it’s the all the little tweaks and enhancements that start to clutter the explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with a high-level insight about where we’re going. <br />\n",
    "Word2Vec은 기계 학습에서 살펴보았던 Trick을 사용한다. <br />\n",
    "우리는 특정 작업을 수행하기 위해 하나의 hidden layer로 구성된 간단한 신경망을 학습시킬 예정이다. <br />\n",
    "그러나 우리는 그 네트워크를 어떤 목적을 위해 학습시키는데 사용하지는 않을 것이다. <br />\n",
    "대신 hidden layer의 가중치들을 Learn 하는 것이 목표이다. <br />\n",
    "이러한 가중치들이 우리가 Learn 하려고 하는 Word vertors이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아마 이 Trick을 Unsupervised feature learning에서 보았을 것이다. <br />\n",
    "Auto Encoder에서는 입력 벡터를 Hidden layer로 압축하고 그것을 다시 원래 크기의 출력 벡터로 Decompress하는 네트워크를 학습한다. <br />\n",
    "학습 후에, 출력 레이어(The decompression step)을 제거하고 오직 Hidden layer만을 사용한다. <br />\n",
    "이 방법은 라벨이 달린 학습 데이터 없이도 좋은 이미지 Feature을 학습하기 위한 좋은 Trick이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fake Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 우리는 위 작업을 수행할 신경망을 구축하기 위한 Fake task에 대해 살펴볼 것이다. <br />\n",
    "그런 다음 나중에 어떻게 간접적으로 우리가 실제로 사용하는 Work vector를 제공하는지 알아볼 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음을 위해 신경망을 학습할 것이다. <br />\n",
    "- 문장 중간에 특정 단어(The input work)가 있으면 근처의 단어를 보고 무작위로 하나를 선택한다.\n",
    "- 네트워크는 우리가 선택한 \"Nearby word\"라는 어휘의 모든 단어에 대한 확률을 알려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Nearby\"라고 하면, 실제로 알고리즘에 대한 \"Window size\" 파라미터가 있다. <br />\n",
    "일반적인 Window size 5라고 하면, 5단어 뒤에 그리고 5단어 앞에 (총 10개)를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 확률은 입력 단어 근처의 각 어휘를 얼마나 쉽게 찾을 수 있는지와 관련있다. <br />\n",
    "예를 들어, 만약 학습된 네트워크에 \"Soviet\"라는 입력 단어를 제공하면 \"Union\"과 \"Russia\"와 같은 단어의 경우 \"Watermelon\"과 \"Kangaroo\"와 같은 관련없는 단어들보다 <br /> \n",
    "출력 확률이 훨씬 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 학습 문서에 있는 단어들의 쌍을 Feeding 함으로써 신경망을 학습시킬 것이다. <br />\n",
    "아래의 예제는 \"The quick brown fox jumps over the lazy dog\"라는 문장에서 취할 수 있는 몇 가지 학습 샘플(단어 쌍)을 보여준다. <br />\n",
    "이 예제에서는 Window size 2를 사용하였다. <br />\n",
    "하이라이트된 파란색은 입력 단어이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크는 각 쌍이 나타나는 횟수로부터 나온 통계를 학습한다. <br />\n",
    "예를 들어, 네트워크는 아마 (\"Soviet\", \"Sasquatch\")보다 (\"Soviet\",\"Union\")의 샘플을 더 많이 얻을 것이다. <br />\n",
    "훈련이 끝난다음 \"Soviet\"라는 입력을 하면 \"Sasquatch\"보다 \"Union\"또는 \"Russia\"에 대한 확률이 훨씬 더 높다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 어떻게 모든 것을 나타낼 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무엇보다도, 신경망에 텍스트 문자열로 단어를 Feed 할 수 없기 때문에, 우리는 단어를 네트워크에 표현할 방법이 필요하다. <br />\n",
    "이를 위해 먼저 학습 문서에서 단어의 어휘를 구축한다. <br />\n",
    "여기서는 우리가 10,000개의 고유한 단어가 있다고 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 \"Ants\"와 같은 입력 단어를 one-hot 벡터로 나타낼 것이다. <br />\n",
    "이 벡터는 10,000개의 Components를 가지며(One for every word in our vocabulary) \"Ants\"라는 단어에 해당하는 위치에 \"1\"을 대응하고 다른 모든 위치에는 0을 대응한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크의 출력은 어휘의 모든 단어에 대해 임의로 선택된 인접 단어가 그 어휘일 확률을 나타내는 단일 벡터 (Also with 10,000 components)이다. <br />\n",
    "아래는 우리의 신경망의 구조이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer 뉴런들에는 활성화 함수가 없지만 출력 뉴런에서는 Softmax를 사용하였다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 쌍에 대하여 이 네트워크를 학습시킬 때, 입력은 입력 단어를 나타내는 One-hot 벡터이고 출력 또한 출력 단어를 나타내는 One-hot 벡터이다. <br />\n",
    "그러나 입력 단어에 대하여 학습된 네트워크를 평가할 때, 출력 벡터는 실제로 확률 분포가 된다. (즉, One-hot vector가 아니라 부동 소숫점들의 묶음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에서, 우리는 300개의 Feature를 가진 단어 벡터를 학습하고 있다고 말할 수 있다. <br />\n",
    "그래서 Hidden layer는 10,000개의 행(One for every word in our vocabulary)과 300개의 열(one for every hidden neuron)로 이루어진 가중치 행렬로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300개의 Feature은 구글 뉴스 데이터 셋에 대하 학습시킨 모델에서 나온 것이다. <br />\n",
    "Feature의 수는 어플리케이션에 따라 조정해야하는 하이퍼 파라미터이다. (즉, 다른 값들을 시도해보고 어떤 값에서 최고의 결과를 얻을 수 있는지 확인해야한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 가중치 행렬의 행이 실제로 단어 벡터가 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 이 모든 것의 최종 목표는 Hidden layer의 가중치 행력을 학습하는 것이다. <br />\n",
    "The output layer we will just toss when we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 질문에 답해보자. <br />\n",
    "\"That one hot vector is almost all zeros... what is the effect of that?\" <br />\n",
    "만약 1x10,000 one-hot 벡터를 10,000x300 행렬로 곱하면, 효과적으로 \"1\"에 대응하는 행렬 행을 선택한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 이 모델의 Hidden layer는 단지 Look-up table로 작동한다는 것을 의미한다. <br />\n",
    "Hidden layer의 출력은 입력 단어에 대한 \"단어 벡터\"이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Ants\"에 대한 1x300의 단어 벡터가 출력 레이어로 Feed 된다. <br />\n",
    "출력 레이어는 Softmax regression classifier 이다. <br />\n",
    "각각의 출력 뉴런들은 0과1사이의 출력을 만들고, 모든 출력 값들의 합은 1이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특히, 각 출력 뉴런은 가중치 벡터를 가진다. <br />\n",
    "이 가중치 벡터를 Hidden layer의 단어 벡터에 곱한 다음 그것의 결과를 exp(x) 함수에 적용한다. <br />\n",
    "마지막으로 출력 합계를 1로 만들기 위하여, 이 결과를 모든 10,000개의 출력 노드의 결과 합계로 나눈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 단어 \"car\"에 대한 출력 뉴런의 출력을 계산하는 예이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 입력 단어에 대한 출력 단어의 Offset에 대하여 아무것도 모른다. <br />\n",
    "It does not learn a different set of probabilities for the word before the input versus the word after. <br />\n",
    "우리의 훈련 자료에서는 'York'라는 단어 앞에 'New'가 온다고 가정해보자. <br />\n",
    "즉, 최소한의 훈련 데이터에 따르면, 'York'의 부근에 'New'가 올 확률은 100%이다. <br />\n",
    "그러나 'York' 부근에서 10 단어를 무작위로 선택하면 'New'는 100%가 아니다. <br />\n",
    "아마 부근에 있는 다른 단어를 선택하였을 수도 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 두 개의 다른 단어가 매우 유사한 \"문맥\"(즉, 어떤 단어가 그 주위에 나타날 가능성이 높음)을 가지고 있다면, 우리의 모델은 이 두 단어에 대해 매우 유사한 결과를 출력해야 한다. <br />\n",
    "만약 두 단어가 유사한 경우라면 네트워크는 두 단어에 대하여 유사한 문맥 예측을 출력으로 나타낸다. <br />\n",
    "그래서, 만약 두 단어가 비슷한 문맥을 가진다면 우리의 네트워크는 이 두 단어에 대해 유사한 단어 벡터를 학습하도록 motivate 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 단어가 비슷한 문맥을 갖는다는 것은 무엇을 의미하는가? <br />\n",
    "\"Intelligent\"와 \"Smart\"같은 동의어는 매우 비슷한 문맥을 가질 것이라고 기대할 수 있다. <br />\n",
    "또는 \"engine\"과 \"transmission\"과 같은 관련 단어는 비슷한 맥락을 가질것이라고 생각할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석 또한 가능하다. <br />\n",
    "네트워크는 \"ant\"와 \"ants\"에 대하여 비슷한 단어 벡터를 학습하게 된다. 왜냐하면 이들은 비슷한 문맥을 가져야 하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ll cover a few additional modifications to the basic skip-gram model which are important for actually making it feasible to train. <br />\n",
    "When you read the tutorial on the skip-gram model for Word2Vec, you may have noticed something–it’s a huge neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제에서, 우리는 300개의 components가 있는 단어 벡터와 10,000개의 단어로 이루어진 어휘를 사용하였다. <br />\n",
    "신경망에는 hidden layer와 output layer라는 두 가지의 가중치 행렬이 있다. <br />\n",
    "이 두 레이어는 각각 300x10,000=3 million 개의 가중치를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사 감소법을 신경망에서 실행하면 가중치가 많아질수록 느려진다. <br />\n",
    "설상가상으로, 많은 가중치들을 조정하느라 생기는 Over fittng을 방지하기 위해서는 매우 많은 양의 학습 데이터가 필요하다. <br />\n",
    "수 백만개의 가중치들 곱하기 수 십억개의 학습 샘플들은 이 모델을 훈련하는 것이 불가능에 가깝다는 것을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec의 저자는 2번째 논문을 통해 위 문제의 해결책을 제시하고자 한다.\n",
    "- 공통 단어 쌍 또는 구문을 단일 단어로 처리한다.\n",
    "- 학습 데이터의 수를 줄이기 위해 자주 Subsampling을 한다.\n",
    "- \"Negative Sampling\"이라는 기법을 이용하여 최적화 목표를 수정한다. <br /> \n",
    "이는 각 학습 Sample이 모델의 가중치들 중 일부만 업데이트 하는 효과를 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈번한 단어 Subsampling과 Negative Sampling을 적용하는 것은 학습과정의 계산 부담을 줄여줄 뿐만 아니라 결과로서 생성되는 단어 벡터의 품질 또한 향상시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Pairs and \"Phrases\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문의 저자는 \"Boston Globe\"(신문)과 같은 단어 쌍은 \"Boston\"과 \"Globe\"라는 개별적인 단어와 훨씬 다른 의미를 가지고 있다고 지적하였다. <br />\n",
    "따라서 텍스트 어디에서나 나타날 수 있는 \"Boston Globe\"는, 단어 벡터 표현에 대하여 단일 단어로 취급해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글 뉴스 데이터 셋으로부터 100 billion 단어를 학습하여 얻어진 모델의 결과를 확인 할 수 있다. <br />\n",
    "기존 모델에 Phrases가 추가되어 어휘의 크기가 3 millions로 증가하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개인적으로 Phrase detection이 그들의 논문의 핵심이라고 생각하지는 않지만 이는 꽤 간단하기 때문에 이에 대하여 좀 더 언급할 예정이다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool의 각 단계에서는 2 단어의 조합만을 살펴보지만, 더 긴 Phrases를 얻기 위하여 여러 번 실행할 수도 있다. <br />\n",
    "따라서 첫 번째 단계에서는 \"New York\"이라는 Phrases를 선택하고 다시 실행하면 \"New York City\"가 \"New York\"과 \"City\"의 조합으로 선택된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 Tool은 두 단어의 각 조합이 학습 텍스트에 나타나는 횟수를 계산한 다음 이 수를 방정식을 사용하여 Phrases로 변환할 단어의 조합을 결정한다. <br />\n",
    " <br />\n",
    "The equation is designed to make phrases out of words which occur together often relative to the number of individual occurrences. <br />\n",
    "이 방정식은 또한 \"and the\" 또는 \"this is\"와 같은 일반적인 단어에서 Phrase를 만드는 것을 피하기 위해 자주 사용되지 않는 단어들로 만들어진 Phrase를 선호한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate Phrase Recognition 전략에 대하여 내가 가졌던 생각은 모든 위키피디아 제목들을 어휘로 사용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling Frequent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 예제는 \"The quick brown fox jumps over the lazy dog.\"라는 문장에서 취할 수 있는 몇 가지 학습 Sample(단어 쌍)을 보여준다. <br />\n",
    "아래 예에서는 Small window 크기 2를 사용하였다. <br />\n",
    "파란색으로 하이라이트된 단어가 입력 단어이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"the\"와 같은 일반적인 단어에는 두 가지 문제가 있다.\n",
    "- 단어 쌍을 볼때 (\"여우\", \"the\")는 \"여우\"의 의미에 대해 많은 것을 알려주지는 않는다. \"the\"는 거의 모든 단어의 문맥에서 나타난다.\n",
    "- 우리는 필요 이상으로 (\"the\", ...) Sample을 갖게 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec은 이 문제를 해결하기 위해 Subsampling 체계를 구현하였다. <br />\n",
    "학습 텍스트에서 만나는 각각의 단어에 대하여, 효과적으로 위와 같은 단어들을 제거할 기회가 있다. <br />\n",
    "우리가 이러한 단어들을 잘라낼 확률은 그 단어의 빈도수와 관련이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 Window size의 크기가 10이고 텍스트에서 특정 인스턴스 \"the\"를 제거하는 경우\n",
    "- 나머지 단어들을 학습하는 동안, \"the\"는 Context window에 나타나지 않는다.\n",
    "- \"the\"가 입력 단어인 경우 10 개 적은 학습 Sample을 가지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 두 가지 효과가 위에서 언급한 두 가지 문제를 해결하는데 어떻게 도움이 되는지 확인해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${w}_{i}$는 단어, $z(w_i)$는 the total words in the corpus that are that word의 비율이다. <br />\n",
    "예를 들어, 만약 \"peanut\"이 1 billion word corpus에서 1,000 번 발생하면 z('peanut')=${10}^{-6}$이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 Subsampling의 발생 정도를 제어하는 'sample'이라는 매개변수가 있으며 기본 값은 0.001 입니다. <br />\n",
    "'sample'의 값이 작을수록 값을 저장할 가능적이 낮다는 것을 의미한다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_i)$는 단어를 저장할 가능성을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림은 위 식을 그래프로 나타낸 것이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 한 단어도 corpus에서 큰 비율을 차지할 수는 없으므로, 우리가 보고자 하는 것은 x-axis에서 상당히 작은 값이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 함수의 몇 가지 흥미로운 점은 아래와 같다. (Default sample 값으로 0.001을 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $P(w_i)=1.0$ (100% chance of being kept) when $z(w_i)$<=0.0026. <br />\n",
    "이는 전체 단어들 중 0.26%이상을 나타내는 단어만 subsampled 된다는 의미이다.\n",
    "- $P(w_i)=0.5$ (50% chance of being kept) when $z(w_i)$=0.00746.\n",
    "- $P(w_i)=0.033$ (3.3% chance of being kept) when $z(w_i)$=1.0 <br />\n",
    "이는 corpus가 $w_i$로만 구성되어있는 경우이다.(이런 경우는 없다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 학습한다는 것은 학습 데이터를 가지고 뉴런의 가중치들을 조정하여 학습 Sample을 더 정확하게 예측하는 것을 의미한다. <br />\n",
    "즉, 각 학습 sample은 신경망의 모든 가중치들을 조정한다. <br />\n",
    "\n",
    "위에서 언급하였듯이, 단어 어휘의 크기는 skip-gram 신경망에 매우 많은 가중치들이 있음을 의미하며, 이 모든 가중치들은 수십 억개의 학습 sample들을 통하여 매우 조금씩 업데이트된다. <br />\n",
    "\n",
    "Negative sampling은 각 학습 Sample이 가중치의 일부분만을 수정하도록 하여 이 문제를 해결한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 쌍(\"fox\", \"quick\")로 네트워크를 학습 할 때 네트워크의 \"label\" 즉 \"correct output\"은 one-hot 벡터이다. <br />\n",
    "즉, \"1\"을 출력하는 \"quick\"에 해당하는 출력 뉴런과 0을 출력하는 다른 수 천개의 출력 뉴런으로 구성되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling을 사용하면, 가중치들을 업데이트 하기 위해 소수의 \"negative\" 단어(예를 들어 5개)를 무작위로 선택한다. <br />\n",
    "문맥에서 \"negative\" 단어는 네트워크가 0을 출력하기 원하는 단어이다. <br />\n",
    "또한 \"positive\" 단어(현재 예제에서는 \"quick\"이라는 단어)에 대한 가중치도  업데이트 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 논문에서는 작은 데이터 셋에서는 5~20 단어를 선택하는 것이 효과적이며, 큰 데이터 셋에서는 2~5 단어를 선택하는 것이 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리 모델의 출력 레이어에는 300x10,000의 가중치 행렬이 있다. <br />\n",
    "따라서 우리는 \"positive\" 단어 (\"quick\")에 대한 가중치와 5개의 다른 단어에 대한 가중치를 업데이트할 것이다. <br />\n",
    "총 6개의 출력 뉴런과 총 1,8000개의 가중치가 있다. <br />\n",
    "이는 출력 레이어의 3 M 가중치의 0.06%에 불과하다. <br />\n",
    "Hidden 레이어에서는 입력 단어의 가중치만 업데이트된다.(Negative sampling을 사용하는지의 여부와는 관계없이)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Negative Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"negative sample\"은 (즉, 출력 0을로 출력 할 5개의 출력 단어)는 \"unigram\" 분포를 사용하여 선택된다. <br />\n",
    "기본적으로 Negative sample로 단어를 선택하는 확률은 빈도와 관련이 있으며, 빈번한 단어는 Negative sample로 선택될 가능성이 더 크다. <br />\n",
    "각 단어에는 3/4 제곱이된 빈도(단어 수)와 동일한 가중치가 부여된다. <br />\n",
    "단어를 선택할 확률은 모든 단어에 대한 가중치의 합으로 나눈 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈도를 3/4 제곱하는 것은 경험적인 것이다. <br />\n",
    "논문에서 위 식이 다른 식들보다 더 뛰어난 성능을 보여준다고 말하고 있다. <br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
