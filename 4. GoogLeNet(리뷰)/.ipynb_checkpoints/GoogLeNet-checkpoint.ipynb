{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2014년 ILSVRC의 우승은 GoogLeNet이 차지하였다. <br />\n",
    "2014년부터 CNN의 구조에 큰 변화가 나타나기 시작한다. AlexNet이나 ZFNet 그리고 CNN의 원조인 LeNet-5의 구조는 매우 단순한 편이며, 전형적인 형태를 취하고 있고 <br />\n",
    "네트워크의 깊이도 10 레이어 미만이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크의 깊이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN의 성능을 향상시키는 가장 직접적인 방법은 네트워크의 크기를 늘리는 것이다. <br />\n",
    "여기서 네트워크의 크기를 늘린다는 것은 단순히 레이어의 수(depth)를 늘리는 것 뿐만 아니라, 각 레이어에 있는 unit의 수(width)도 늘리는 것을 의미한다. <br />\n",
    "특히 ImageNet 데이터와 같이 대용량 데이터를 이용해 학습을 하는 경우는 거의 필수적이다. <br />\n",
    "아래의 그림은 image classification의 성능 향상을 위해서, CNN의 구조가 어떻게 바뀌고 있는지 보여주는 그래프이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2013년까지는 CNN 네트워크의 깊이가 10 레이어 미만이었지만, 2014년도의 GoogLeNet과 VGGNet은 각각 22 레이어와 19 레이어로 더 깊어지게 된다. <br />\n",
    "또한 top-5 error rates도 각각 6.7%와 7.3%로 낮아지게 된다. <br />\n",
    "AlexNet 이후 불과 2년만에 error rates를 약 10%정도 낮추게 된다. 2015년 우승을 한 ResNet은 네트워크의 깊이가 152 레이어로 더욱 깊어지게 되며, <br />\n",
    "top-5 error rates도 3.57%로 더욱 낮아지게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크가 깊어질 때의 Side Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크의 크기를 늘리면 성능을 더 높일 수 있지만 다음과 같은 문제들을 야기할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 네트워크가 커질수록 파라미터의 수가 증가하게 되며 네트워크가 과적합에 빠질 가능성이 높아진다. 이는 특히 학습에 사용할 데이터 양이 제한적인 경우에 더 심각하다. <br />\n",
    "- 네트워크가 커질수록 그만큼 연산량이 늘어나게 된다. 예를 들면, filter의 갯수가 증가하게 되면 연산량은 제곱 수준으로 늘어나게 된다. <br />\n",
    "연산 능력이 뛰어난 GPU를 사용하더라도 연산량 증가는 심각한 문제를 초래할 수 있다. <br />\n",
    "- 학습이 잘못된 방향으로 흘러 feature map의 feature들이 특정한 무리로 편향되면, 네트워크의 크기를 늘렸음에도 불구하고 최적의 결과를 얻지 못할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet보다 레이어의 수가 작은 AlexNet을 살펴보자. AlexNet은 파라미터의 갯수가 60 millions 이고 약 630 millions의 connections로 이루어져있으며, <br />\n",
    "엔비디아의 GTX580 dual GPU를 이용하였음에도 불구하고 학습 시간에 일주일이 넘게 소요되었다. <br />\n",
    "단순히 네트워크를 깊게 만든다면, 파라미터의 갯수가 더욱 많아지게 될 것이고 connection도 엄청나게 많아져서, 학습에 필요한 시간도 더욱 길어지게 될 것이다. <br />\n",
    "또한 학습이 끝난 후 파라미터들의 값이 정해지고나서도 테스트를 하면서 실제 연산을 할 때의 연산량도 무시할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 모바일이나 embedded 시스템에서 CNN을 활용하고자 한다면, 연산 능력이나 메모리 사용등에서 PC를 사용할 때보다 훨씬 제한적일 수 밖에 없기 때문에, <br />\n",
    "단순히 네트워크를 깊게 만드는 것이 아니라 무언가 구조적인 고민이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet과 Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 구글의 GoogLeNet 소개 자료에 나오는 그림이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글이 발표한 Inception의 기본 구조는 아래의 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 레이어에서 서로 다른 크기를 같은 convolution filter를 적용하여 다른 scale의 feature를 얻을 수 있도록 하였다. <br />\n",
    "그림에서처럼 1x1 convolution을 적절히 사용하여 차원을 줄이고 네트워크가 깊어졌을 때 연산량이 늘어나는 문제를 해결하였다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet은 구글의 연구팀이 inception 모듈을 고안한 뒤에 2014년 ILSVRC에 참가하기 위한 버전으로 내놓은 것이며, inception 구조는 다양한 형식으로 적용이 가능하다. <br />\n",
    "AlexNet과 GoogLeNet을 비교한 그림은 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet의 놀라운 점은 네트워크의 깊이는 훨씬 깊은데 파라미터의 수는 1/12 수준이고 전체 연산량의 숫자도 AlexNet에 비해 적다는 것을 알 수 있다. <br />\n",
    "GoogLeNet에는 총 9개의 inception 모듈이 적용되어 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Inception 모듈 및 전체 구조에 관련된 부분은 싱가폴 국립 대학의 \"Min Lin\"이 2013년에 발표한 \"Network in Network\"구조를 더욱 발전시킨 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIN(Network In Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NIN은 말 그대로 네트워크 속의 네트워크를 뜻한다. <br />\n",
    "일반적인 CNN 구조는 feature extraction 부분(conv + pooling)과 Classifier(FC 레이어)로 구성된다. <br />\n",
    "Conv 레이어와 pooling 레이어를 번갈아 사용하는 레이어 그룹을 여러 개 사용하여 feature를 추출하고, <br />\n",
    "마지막 단에서 FC 레이어를 이용하여 최종적으로 feature vector를 classify 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIN의 설계자는 CNN의 Conv 레이어는 local receptive field에서 feature를 추출해내는 능력은 우수하지만, filter는 linear 연산을 때문에 non-linear한 feaure를 추출하기엔 <br />\n",
    "어려움이 있으므로 이를 극복하기 위해 feature map의 갯수를 늘려야 하는 문제에 주목하였다. 하지만 filter의 갯수가 늘어나게 되면 연산량이 늘어나는 문제가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 NIN의 설계자는 local receptive field 안에서 좀 더 feature를 잘 추출해낼 수 있는 방법을 연구하였으며, 이를 위해 micro neural network를 설계하였다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이들은 convolution 연산을 수행하기 위한 각각의 filter에 MLP(Multi-Layer Perceptron) 레이어를 추가하여 feature를 추출하도록 하였다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP를 사용하였을 때의 장점은 단순히 convolution 연산을 하는 것 보다 non linear한 feature를 잘 추출해 낼 수 있다. <br />\n",
    "또한 1x1 convolution 연산을 이용하여 feature map의 수를 줄일 수 있도록 하였으며, 이 기법은 GoogLeNet의 인셉션에 그대로 적용된다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet에서도 인셉션 모듈을 총 9개 사용하기 때문에 개념적으로는 NIN과 연결이 되어 있다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 MLP Conv 레이어를 3개 사용한 구조이다. <br />\n",
    "NIN 구조가 기존 CNN과 또 다른 점은 CNN의 최종단에 있는 FC 레이어가 없다는 점이다. FC 레이어 대신에 최종단에 \"Global average pooling\"을 사용하였다. <br /> 이는 앞 레이어에서 효과적으로 feature들을 추출하였기 때문에, 마지막 레이어에서는 Average pooling 만으로 classifier 역할을 어느정도 할 수 있기 때문이다. <br /> \n",
    "따라서 overfitting의 문제를 줄일 수 있고 연산량 또한 줄어드는 효과도 얻을 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 Convolution 이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1x1 convolution을 하는 결정적인 이유는 차원을 줄이는 것이다. <br />\n",
    "GoogLeNet의 논문에 소개되어 있는 Hebbian principle(Neurons that fire together, wire together)에 의해 차원을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1x1 convolution을 수행하면, 여러 개의 feature map으로부터 비슷한 성질을 갖는 것들을 묶어낼 수 있고, 결과적으로 feature map의 숫자를 줄일 수 있으며, <br />\n",
    "연산량을 줄일 수 있게 된다. 또한 연산량이 줄어들게 되면 네트워크를 더 깊게 만들 수 있는 여지가 생기게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 C2 > C3 의 관계가 만들어지면 차원을 줄이는 것과 같은 효과를 얻을 수 있기 때문에 GoogLeNet을 포함한 최신 CNN 구조에서는 1x1 convolution을 많이 사용한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1x1 convolution을 1-later-fully-connected neural network라고도 하는데, 그 이유는 1x1 convolution이 fully-connected와 같은 방식이기 때문이다. <br />\n",
    "만약에 입력 feature map C2의 갯수가 4이고, 출력 feature map C3의 갯수가 2인 경우를 생각해보면, 1x1 convolution은 아래의 그림과 같이 표현할 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 보면 4개의 feature map으로부터 입력을 받아, 학습을 통해 얻어진 파라미터를 통해 2개의 feature map으로 결정이 된다. <br /> \n",
    "즉, 차원이 줄어들게 되며, 이를 통해 연산량이 줄어든다. 또한, 뉴런에서 활성화 함수로 ReLU를 사용하게되면 non linearity 또한 얻을 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글은 인셉션에 대한 개발을 하면서 NIN 구조를 많이 참고하였다. Local receptive field에서 더 다양한 feature를 추출하기 위해 여러 크기의 convolution filter를 <br />\n",
    "병렬적으로 활용하려고 하였다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 1x1, 3x3, 5x5 및 3x3 max pooling을 나란히 놓는 구조를 고안하였다. 이는 다양한 scale의 feature를 추출하기에는 적합한 구조이지만 3x3과 5x5 convolution은 <br /> \n",
    "연산량의 관점에서 보면 expensive unit이다. 네트워크의 깊이가 깊지 않을 때는 쿤 문제가 되지 않을 수 있으나, 네트워크의 깊이와 넓이가 넓어지는 GoogLeNet의 <br /> 구조에서는 치명적일 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 아래의 그림과 같이 3x3 convolution과 5x5 convolution의 앞에 1x1 convolution을 cascade 구조로 두고, 1x1 convolution을 통해 <br />\n",
    "feature map의 갯수(차원)를 줄이게 되면 feature 추출을 위한 여러 scale을 확보하면서도, 연산량을 어느정도 수준으로 맞출 수 있게 된다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet이 22 레이어까지 깊어질 수 있는 것도 1x1 convolution을 통해 연산량을 조절할 수 있었기 때문이라고 볼 수 있다. <br />\n",
    "3x3 max pooling에 대해서도 1x1 convolution을 둔다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIN에서는 MLP를 이용하여 non linear feature를 얻어내는 것에 주목하였지만, MLP는 결국 fully connected neural network의 형태이다. <br />\n",
    "반면 GoogLeNet은 기존 CNN의 구조에서 크게 벗어나지 않으면서도 효과적으로 feature를 추출할 수 있게 되었다. <br />\n",
    "요약하면, 인셉션 모듈을 통해 GoogLeNet은 AlexNet에 비해 네트워크의 깊이는 훨씬 깊지만 파라미터의 수는 1/12 수준이고 전체 연산량도 AlexNet에 비해 <br /> \n",
    "훨씬 적은 구조를 가지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014년에 발표된 GoogLeNet과 VGGNet 등에서 가장 주목할 만한 특징은 네트워크가 깊어지고 넓어졌다는 점이며, 이후 CNN 구조의 대세를 이루게 된다. <br /> \n",
    "네트워크를 깊고 넓게 만들수록 학습 능력이 증가하기 때문이다. 하지만, 네트워크가 깊어지고 넓어지게 적절한 하이퍼 파라미터의 설정이나 초기값 설정이 없다면  <br />\n",
    "과적합 문제에 빠질 가능성이 훨씬 증가하게 되며, 연산량의 문제로 인해 학습 시간 또한 증가하게되고, 그로 인해 최적의 결과를 얻어내지 못할 수도 있게 된다. <br /> \n",
    "GoogLeNet 설계자들은 단순하게 하이퍼 파라미터나 초기값을 잘 설정하는 것만으로는 한계가 있다는 점을 이해하고 있었다. <br /> \n",
    "따라서 전체 구조적인 측면을 주목 하였으며, 다양한 CNN 구조 실험을 통해, 연산량을 유지하면서 네트워크의 깊이와 넓이를 증가시킬 수 있는 구조를 얻게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet의 핵심 철학 및 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet의 핵심 철학은 주어진 하드웨어 자원을 최대한 효율적으로 이용하여 학습 능력을 극대화 할 수 있는 깊고 넓은 망을 갖는 구조를 설계하는 것이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 위해 인셉션이라는 독특한 모듈을 만들어 내었다. <br />\n",
    "인셉션 모듈에 있는 다양한 크기의 convolution filter(그림에서 파란색 부분)을 통해 다양한 scale의 feature를 효과적으로 추출하는 것이 가능해졌다. <br />\n",
    "또한 인셉션 모듈 내부의 여러 곳에서 사용되는 (위 그림의 노란색 부분) 1x1 convolution을 통해 연산량을 크게 줄였다. <br /> \n",
    "이는 결과적으로 네트워크의 넓이와 깊이를 증가시킬 수 있는 기반이 되었다. 이 인셉션 모듈을 통해 NIN(Network in Network) 구조를 갖는 deep CNN 구현이 가능하게 되었다.  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인셉션은 모듈 개념이기 때문에, CNN을 적용하고자 하는 목적이나 분야에 따라 그 조합을 다르게 가져갈 수 있다. <br />\n",
    "GoogLeNet에는 아래의 그림과 같이 총 9개의 인셉션 모듈이 적용되었다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 빨간색 동그라미가 인셉션 모듈에 해당된다. 위 그림에서 파란색 Unit은 convolution을 의미하고 빨간색 Unit은 max pooling을 의미하며, <br />\n",
    "노란색 Unit은 Soft max, 녹색은 기타 function을 의미한다. 인셉션 모듈을 나타내는 동그라미 위에 있는 숫자는 각 단계에서 얻어지는 feature map의 수를 나타낸다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet의 각 레이어의 구조는 아래의 표와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Patch size/stride: filter의 크기와 stride 간격을 말한다. <br />\n",
    "\n",
    "- Output size: 출력으로 얻어지는 feature-map의 크기 및 개수를 나타낸다. <br /> \n",
    "\n",
    "- Depth: 연속적인 Conv 레이어의 개수를 의미한다.\n",
    "\n",
    "- 1x1: 1x1 convolution을 의미하며, 그 행에 있는 숫자는 1x1 convolution을 수행한 뒤 얻어지는 feature-map의 개수를 말한다. \n",
    "\n",
    "- 3x3 reduce: 이것은 3x3 convolution 앞쪽에 있는 1x1 convolution 을 의미한다.\n",
    "\n",
    "- 3x3: 1x1 convolution에 의해 차원이 줄어든 feature map에 3x3 convolution을 적용하는 것을 의미한다.\n",
    "\n",
    "- 5x5 reduce: 해석 방법은 “3x3 reduce”와 동일하다.\n",
    "\n",
    "- 5x5: 해석 방법은 “3x3”과 동일하다\n",
    "\n",
    "- Pool/proj: 이 부분은 max-pooling과 max-pooling 뒤에 오는 1x1 convolution을 적용한 것을 의미한다.\n",
    "\n",
    "- Params: 해당 layer에 있는 파라미터의 개수를 나타내며, 입력 feature map과 출력 feature-map의 갯수와 크기에 비례한다. \n",
    "\n",
    "- Ops: 연산의 수를 나타낸다. 연산의 수는 입력 feature map과 출력 feature-map의 갯수와 크기에 비례한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인셉션 3(a)에는 256이라는 빨간색 숫자가 적혀 있는데, 이것은 인셉션 3(a)를 통해 총 256개의 feature map이 만들어졌다는 뜻이며, 이것은 1x1 convolution을 통해 64개, <br /> \n",
    "1x1과 3x3 연속 convolution을 통해 128개, 1x1과 5x5 연속 convolution을 통해 32개, max-pooling과 1x1 convolution을 통해 32개를 적용하여 <br /> 도합 256개의 feature-map을 얻게 되었다는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 3x3보다는 5x5 convolution을 통해 얻는 feature-map의 갯수가 작은 이유는 5x5 convolution이 훨씬 연산량을 많이 필요로 하기 때문이며, <br /> 입력 이미지의 크기가 이미 28x28로 줄어든 상황에서는 3x3으로 얻을 수 있는 feature가 5x5로 얻을 수 있는 feature보다 많기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림의 GoogLeNet 구조를 보면 Auxiliary classifier라고 불리는 독특한 Unit이 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/14.png\" width=900 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 CNN 구조에서는 볼 수 없었던 독특한 구조이다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크가 깊어지면서 생기는 큰 문제 중 하나는 그라디언트 vanishing 문제이며, 이로 인해 학습 속도가 아주 느려지거나 혹은 과적합 문제가 발생하기도 한다. <br />\n",
    "뉴럴 네트워크에서는 최종 레어어의 error를 back propagation을 하면서 파라미터 값을 갱신한다. <br /.\n",
    "그런데 그라디언트 값들이 0 근처로 가게 되면, 파라미터의 변화가 별로 없어 학습 속도가 아주 느려져서 학습 결과가 더 나빠지는 현상이 발생할 수 있다. <br />\n",
    "활성화 함수로 Sigmoid를 사용하는 경우, Sigmoid 함수의 특성상 일부 구간을 제외하면 미분값이 거의 0으로 수렴하기 때문에 출력 error의 크기와 상관없이 학습 속도가 느려진다. <br />\n",
    "Cross entropy 함수를 사용하면 좀 더 개선은 되지만 본질적인 문제가 해결되는 것은 아니다. <br />\n",
    "최근 DNN(Deep Neural Network)에서는 활성화 함수로 ReLU를 사용한다. <br />\n",
    "Sigmoid나 Cross entropy 함수를 사용할 때보다는 더 많은 이점이 있지만 ReLU 역시 여러 레이어를 거치면서 작은 값들이 계속 곱해지다 보면, 0 근처로 수렴되면서 <br />\n",
    "역시 Vanishing 그라디언트 문제에 빠질 수 있다. 이는 네트워크가 깊어질수록 가능성이 더 커진다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet은 이 문제를 극복하기 위해 Auxiliary classifier를 중간에 2곳 두었다. <br />\n",
    "학습을 할 때는 이 Auxiliary classifier를 이용하여 Vanishing 그라디언트 문제를 피하고, 수렴을 더 좋게 해주면서 학습 결과가 향상된다. <br />\n",
    "이 Auxiliary classifier는 GoogLeNet 논문에는 자세한 설명이 없지만, 이 후 DNN을 연구하는 사람들이 이 구조에 대한 개선이나 이론적인 설명을 위한 논문을 발표하였으며 <br />\n",
    "대표적인 논문은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Deeply supervised nets (C.Y. Lee, S. Xie 등) <br />\n",
    "> Training Deeper Convolutional Networks with Deep SuperVision(Liwei Wang, Chen-Yu Lee 등) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 그림은 Liwei Wang의 논문에 나오는 DNN의 구조이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/15.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X4의 위치에 Super Vision이라고 불리는 Auxiliary classifier를 배치하고, back propagation 시에 X4 위치에서는 <br />\n",
    "Auxiliary classifier와 최종 출력으로부터 정상적인 back propagation 결과를 결합시킨다. <br/ >\n",
    "이렇게 되면, Auxiliary classifier의 back propagation 결과가 더해지기 때문에 X4의 위치에서 그라디언트가 작아지는 문제를 피할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet에서는 어느 위치에 Auxiliary classifier를 놓을 것인지, 어떤 결과를 얻었느지 명확하게 밝히지 않았지만, Liwei 연구팀은 초기(10~50)번 정도의 <br />\n",
    "Iteration을 통해 그라디언트가 어떻게 움직이는지 확인하고, 특정 위치에 Auxiliary classifier를 추가하는 것이 좋다고 논문에서 밝혔다. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/16.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 왼쪽 그림은 Auxilary classifier가 없는 경우 앞 단의 그라디언트가 현저히 작아지는 것을 보여준다. <br />\n",
    "오른쪽 그림은 Auxilary classifier가 없는 경우는 파란색 점선과 같이 iteration이 진행될 수록 그라디언트가 0에 근접한 값을 갖는 반면, <br />\n",
    "빨간색 선은 Auxilary classifier에 의해 그라디언트가 다시 증가하게 되는 것을 보여주며, 결과적으로 더 안정적인 학습 결과를 얻을 수 있게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 2015년에 GoogLeNet의 첫번째 저자 Szegedy가 발표한 논문 “Rethinking the Inception Architecture for Computer Vision” 에서 다시 Auxiliary classifier에 <br />\n",
    "대하여 언급되는데, 이 논문에 따르면, Auxiliary classifier가 “Regularizer”와 같은 역할을 하며, 중간의 side branch에 있는 Auxiliary classifier에도 <br /> \n",
    "batch normalize 기법을 적용하거나 drop-out layer를 갖고 있으면 결과가 더 좋아진다. <br />\n",
    "학습이 끝나고, 학습된 DNN을 이용할 때는 Auxiliary classifier는 삭제한다. <br />\n",
    "즉, Auxiliary classifier는 학습을 도와주기 위한 도우미 역할만을 하고, 학습을 통해 결과를 얻게 되면, 본래의 역할을 다했기 때문에 제거한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorizing Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 크기를 갖는 convolution filter를 인수분해하면 작은 filter 여러 개로 구성된 deep network를 만들 수 있으며, <이렇게 하면 파라미터의 수가 더 줄어들면서 네트워크는 <br /> \n",
    "깊어지는 효과를 얻을 수 있다. 아래의 그림은 5x5 convolution filter를 두 개의 3x3 convolution filter로 구현할 경우를 보여준다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/17.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 5x5 convolution은 두 개의 3x3 convolution을 사용해 구현이 가능하며, 이 경우 파라미터의 수는 5x5 => (3x3+3x3)로 절감시킬 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 그림은 위 방식을 적용하여 원래의 인셉션 구조를 변형시킨 결과이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/18.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7x7 convolution의 경우도 마찬가지로 세 개의 3x3 convolution으로 대응이 가능하며, 이 경우는 7x7 => (3x3+3x3+3x3)로 파라미터를 절감할 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5x5나 7x7 convolution을 여러 레이어의 3x3 convolution으로 대응하는 것과 같이 대칭을 유지하는 방식으로도 인수분해가 가능하지만, <br />\n",
    "대칭을 유지하지 않고 row방향 혹은 column방향으로 인수분해 하는 것도 가능하다. <br />\n",
    "아래의 그림은 3x3 convolution을 1x3 convolution과 3x1 convolution으로 분해한 것이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/19.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 되면 파라미터의 수는 3x3 => (1x3+3x1)로 절감시킬 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비슷하게 (n)x(n) convolution은 1x(n) 과 (n)x1로 분해가 가능하며, n이 클수록 파라미터 절감 효과가 커진다.\n",
    "이를 인셉션 구조에 표현하면 아래의 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/20.png\" width=350 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 n=3이면 앞서 본 인셉션 모듈과 동일하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 큰 필터를 균일한 크기의 3x3으로 표현하는 것은 VGGNet의 핵심 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "통상적으로 CNN의 구조를 보면 Conv 레이어 뒤에 pooling 레이어를 두고 feature map의 gride(해상도)를 줄이는 것이 일반적이었다. <br />\n",
    "하지만 인셉션 모듈을 보면 여러 개의 Conv 레이어와 pooling 레이어가 나란히 있는 독특한 구조이다. <br />\n",
    "이런 구조는 왜 만들어졌으며 이를 통해 어떻게 grid의 크기를 효과적으로 줄일 수 있는지 알아보자. <br /> \n",
    "또한 convolution filter의 인수분해와 기타 최적화 기술을 이용하여 인셉션 모듈을 개선하는 방법에 대해 살펴보고 이를 통해 이루어진 성능의 개선을 살펴보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 효과적으로 해상도(grid size)를 줄이는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 grid의 크기를 줄이는 대표적인 방법은 convolution을 수행할 때 stride를 1 이상의 값으로 설정하거나 pooling을 사용하는 것이다. <br />\n",
    "전형적인 CNN 구조에서는 Conv 레이어를 이용하여 이전 feature map으로부터 의미있는 특징을 추출하며, 이 때 convolution filter의 갯수는 특징들을 <br />\n",
    "잘 추출할 수 있도록 충분해야 한다. 그리고 다음 단에 오는 pooling 레이어를 이용하여 feature map의 크기를 줄이는 것이 일반적인 방식이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/21.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서는 convolution 대신 인셉션으로 표시되어 있지만, 이것을 convolution이라고 생각해도 큰 차이는 없다. <br />\n",
    "만약 35x35 크기의 320개의 feature map을 입력으로 하여 17x17 크기의 640개의 feature map을 얻고자 한다면 위 두 방식중 어느 쪽이 더 효과적으로 <br />\n",
    "grid의 크기를 줄이는 방식일까?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 왼쪽 방식은 35x35x320 feature map에 먼저 pooling을 적용하여 크기를 절반으로 줄인다. <br /> \n",
    "그리고 뒤에 Inception을 적용하여 17x17 크기의 640개 feature-map을 얻었다. <br /> \n",
    "연산량 관점에서만 보면 이 방식이 효율적인 것처럼 보이지만, Pooling 단계를 거치면서, 원래 feature map에 있는 숨어 있는 정보(representational concept)가 <br /> \n",
    "사라지게 될 가능성이 있으므로 성능 및 효율성의 관점에서 보면 최적이라고 보기는 어렵다. <br /> \n",
    "반면에 오른쪽은 Inception module을 먼저 적용하여 640개의 feature map을 얻은 후에 pooling을 적용하여 feature map의 크기를 줄였다. <br />\n",
    "이 경우에는 큰 크기의 feature map에 Inception을 적용하였기 때문에 연산량의 관점에서 보면, 결과적으로 대략 4배 정도가 많은 셈이 된다. <br />\n",
    "하지만 feature map의 크기를 줄이기 전에 Inception을 적용하였기 때문에 숨은 특징을 더 잘 찾아낼 가능성은 높아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면 좀 더 효과적으로 grid의 크기를 줄일 수 있는 방법은 무엇일까? <br />\n",
    "Szegedy(GoogLeNet)의 설계자 중 한 명은 자신의 논문(Rethinking the inception architecture for computer vision)에서 아래의 그림과 같은 구조를 제시한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/22.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 왼쪽은 구조가 인셉션 모듈과 비슷하다는 것을 발견할 수 있을 것이다. (다른 점이 있다면, 최종단에 stride 값을 \"1\"이 아니라 \"2\"로 적용했다는 점이다) <br />\n",
    "Pooling 레이어 및 convolutional 레이어를 나란하게 배치하고, 최종 단에 stride 2를 적용하게 되면, <br />\n",
    "결과적으로는 5x5 및 3x3 convolution을 통해 local feature를 추출함과 동시에 stride 2를 통해 출력되는 feature map의 grid 크기를 줄이고, <br />\n",
    "또한 pooling layer를 통해 출력되는 feature map의 grid 크기를 줄여 그 결과들을 concat하는 방식이다. <br />\n",
    "오른쪽은 왼쪽보다는 좀 더 단순한 방법으로 stride 2를 갖는 convolution을 통해 320개의 feature map을 추출하고 나란히 배치된 pooling layer를 통해 <br /> \n",
    "다시 320개의 feature map을 추출하여 이를 concat 함으로서 효율성과 연산량의 절감을 동시에 달성할 수 있게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014년 ILSVRC를 참가할 당시에는 구글은 전년도 ZFNet의 결과보다 거의 2배 정도의 성능을 얻었기 때문에, 그리고 사람들이 식별할 수 있는 수준에 육박했기 때문에 <br /> \n",
    "Inception-V1 구조에 만족하였다. 하지만, 불과 1년 후 2015 ILSVRC에서 마이크로소프트의 ResNet이 GoogLeNet 결과보다 거의 2배 좋은 성능으로 우승을 한다. <br />\n",
    "이 결과에 고무되어 Inception-V2 및 Inception-V3가 나오게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014년에 발표한 VGGNet은 GoogLeNet과 거의 유사한 성능을 보이면서, 구조도 3x3 convolution만을 사용하는 단순한 구조로 유명하다.\n",
    "여기에서 힌트를 얻어 convolution filter에 대한 인수분해를 통해 네트워크를 더 깊게 만들면서도 효과적으로 연산량은 더 절감할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18-layer의 VGGNet은 22-layer의 GoogLeNet보다 연산량이 3배 가량 많았기 때문에, 구글의 설계 철학과 맞지 않는다. <br />\n",
    "따라서 구글은 단지 인수분해의 개념만을 가져왔다. 본래의 인셉션 모듈에 인수분해 방식을 사용하여 개선하고, GoogLeNet 앞 단에 있던 7x7 convolution 등을 <br /> \n",
    "인수분해를 통해 작은 크기의 multi 레이어 구조로 개선하였다. 아래 표는 Inception-V2의 레이어 구조를 보여주는 표이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/23.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014년 GoogLeNet에서는 입력 이미지로 224x224x3 크기를 사용하였지만, 2015년 Inception-V2 사용한 구조에서는 입력 이미지를 299x299x3의 크기를 사용하였다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2014년 GoogLeNet의 앞 단 구조는 아래 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/24.png\" width=100 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맨 앞 단은 stride 2값을 적용한 7x7 convolution 뒤에 max pooling을 적용하여 56x56x64 크기의 feature map이 출력된다. <br />\n",
    "다음 단에서는 1x1 convolution 및 3x3 convolution, max pooling을 통해 얻어진 28x28x192 feature map을 인셉션 모듈의 입력으로 사용한다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, Inception-V2를 적용한 2015년 구조에서는 7x7 convolution은 3개의 3x3 convolution으로 레이어가 더 깊어지게 되었으며 pooling을 통해 <br /> \n",
    "73x73x64 크기의 feature-map이 얻어지게 된다. 다음은 3개의 convolution을 통해 최종적으로 35x35x288 크기의 feature map을 얻는다. <br />\n",
    "2014년 구조에서는 1x1 convolution, 3x3 convolution, max pooling을 거쳤지만, 2015년 구조는 3개의 convolution으로 구현을 하였고, <br /> 중간 과정에 stride 2를 적용하여 pooling의 효과를 얻을 수 있도록 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 단에는 인셉션 모듈을 적용하는 것은 비슷하나, 맨 앞 단의 인셉션 모듈의 개수가 2개에서 3개로 늘어났으며, 적용하는 인셉션 모듈의 구조를 개선하여 <br />\n",
    "네트워크는 더 깊어지면서 연산량은 절감할 수 있게 되었다. 최종단의 구조도 2014년 구조와 비슷하기는 하지만, feature map의 개수가 좀 더 많아졌다는 점이 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 구조 변화를 통해 22개의 layer를 갖던 2014년 구조에 비해, 총 42개의 layer로 깊어지게 되었지만, 연산량은 2.5배 늘어난 수준으로 효율성을 보인다. <br />\n",
    "그 결과는 아래 표와 같다.\n",
    "___\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/25.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 표에서 GoogLeNet의 error rate가 기존의 GoogLeNet의 결과보다 높은 이유는 data augmentation 기법을 적용하지 않고 single-crop 적용하였기 때문이다. <br />\n",
    "위의 표에는 Inception-V2를 다양하게 적용했을 때의 결과를 보여주고 있으며, regularization 효과를 극대화 시키기 위해 batch normalized auxiliary classifier를 <br />\n",
    "적용하면, 성능이 5.6%까지 좋아진다는 것을 확인할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muiti-crop을 144개까지 적용하고, Inception-V2의 성능을 더 극대화 시킨 Inception-V3 구조에서는 top-1 error율이 4.1%까지 떨어져 아주 우수한 성능을 보이게 된다. <br />\n",
    "이처럼 단순하게 pooling 레이어를 적용하는 것보다는 convolution 레이어와 같이 나란히 적용하는 것이 효과적이다. <br />\n",
    "또한 convolution filter 에 대한 인수분해 방식을 적용하고, 앞 뒤 일부 구조 및 feature map의 개수를 조정하는 것만으로도 성능을 상당히 개선할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object classification은 이미지 내에 특정 대상(object)이 있는지 여부를 가려내는 것으로 2012년 AlexNet이 발표되면서 거의 매년 성능이 2배 정도씩 좋아지는 경향을 보이고 있다. 하지만, object detection은 이미지 내에 특정 대상이 존재하는지 여부를 판단하는 것 뿐만 아니라, 대상의 정확한 위치까지 파악을 하고 그것을 bounding box라고 부르는 사각형 영역으로 구분하는 것까지 수행을 하여야 하기 때문에 classification에 비해 훨씬 어렵다고 볼 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/26.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection은 machine learning 뿐만 아니라, computer vision 관련 지식도 같이 필요로 하기 때문에, classification에 비해서는 상대적으로 어렵다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2013년 버클리 대학교의 Ross Girshick 팀이 R-CNN(Regions with CNN features)이라는 방법을 발표하면서, 이전 detection 알고리즘에 비해 <br /> \n",
    "2배 이상의 성능 향상이 이루어졌다. ILSVRC에서는 classification 성능뿐만 아니라, detection 성능도 같이 순위를 매긴다. <br /> \n",
    "GoogLeNet 설계자들은 이미 앞서 살펴본 것처럼 인셉션 모듈이라는 독특한 유닛을 이용하여 classification의 성능 향상을 도모하였으며, 전년도에 발표된 <br /> \n",
    "Girshick의 R-CNN 개념을 적용하여 detection 분야에서도 1위를 차지한다. <br /> \n",
    "같은 해에 버클리 팀도 참여를 하였지만 mAP(mean Average Precision)에서 구글은 버클리 팀을 큰 폭으로 따돌리며 여유 있게 우승을 한다. <br />\n",
    "이는 버클리 팀은 AlexNet을 기반으로 사용하였지만, 성능면에서 GoogLeNet이 훨씬 뛰어났기 때문이라고 생각한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN(Regions with CNN features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNN 알고리즘이 발표되기 이전에 대부분 object detection에 주로 사용되던 방법은 SIFT(Scale Invariant Feature Transform)이나 HOG(Histogram of Gradient)에 <br />\n",
    "기반한 알고리즘이다. 이미지에 존재하는 low-level feature에 기반하기 때문에 성능상의 한계가 분명히 존재하며, 이를 보완하기 위해 여러 알고리즘을 섞어 사용하였다. <br />\n",
    "하지만 성능상의 한계로 인해 일정 정도 이상의 성능을 얻을 수 없었다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/27.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버클리의 연구팀은 그동안 DPM이나 HOG 등을 이용한 detection 연구를 꾸준히 해왔던 팀이며, 2012년 AlexNet의 연구 결과에 자극을 받아, <br /> \n",
    "CNN을 이용해 detection 연구를 하는 쪽으로 방향을 선회하였다. R-CNN은 위 그림과 같이 입력 영상으로부터 약 2000개의 후보 영역을 만든다. <br /> \n",
    "이 때 사용하는 방법은 Selective search 방법(상세 내용은 Selective Search for Object Recognition 논문 참고)을 적용하여 후보 영역을 선정한다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selective search는 Uijlings가 처음 발표를 하였으며, segmentation의 장점과 exhaustive search의 장점을 골고루 활용을 하였으며, <br /> \n",
    "영상 속에 있는 color나 texture 등 단순한 정보뿐만 아니라 영상 속에 내재된 계층 구조도 같이 활용을 한다. <br />\n",
    "Selective search를 통해 후보 영역을 선정하고나면, AlexNet은 224x224 크기의 이미지를 받아들이도록 되어 있기 때문에, 해당 영역을 warping이나 crop을 <br /> \n",
    "사용하여 224x224 크기로 만들고, 이것을 AlexNet을 약간 변형한 CNN에 인가하고 최종 출력에서 해당 영상을 대표할 수 있는 CNN feature vector를 얻어낸다. <br />\n",
    "그런 다음 linear SVM을 이용해 해당 영역을 분류한다. 결과적으로 보면, Computer Vision 관련 기술과 CNN 기술을 결합하여 뛰어난 성과를 얻게 된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN의 성능 향상과 학습 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버클리 팀은 detection 성능 평가에 PASCAL VOC(Visual Object Class)를 사용하였다. <br /> \n",
    "PASCAL VOC의 경우 라벨이 붙은 데이터의 양이 ILSVRC보다 상대적으로 적었기 때문에 ILSVRC 결과를 사용하여 CNN을 pre-training을 하였다. <br /> Pre-training에는 bounding box를 사용하지는 않았고, 단지 label 있는 ILSVRC data를 이용해 CNN에 있는 파라미터들이 적절한 값을 갖도록 하였다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 warped VOC를 이용해 CNN을 fine tuning을 한다. 이 때는 ground truth 데이터와 적어도 0.5 IoU(Intersection over Union: 교집합/합집합) 이상 되는 <br /> \n",
    "region 들만 postive로 하고 나머지는 전부 negative로 하여 fine tuning을 시행한다. <br /> \n",
    "이 때 모든 class에 대해 32개의 positive window와 96개의 background window를 적용하여 128개의 mini batch로 구성을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 linear classifier의 성능을 개선을 위해 hard negative mining 방법을 적용하였다. <br />\n",
    "위 과정은 아래 그림과 같이 표현이 가능하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/28.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNN을 적용하게 되면 기존 방법에 비해 아래 표와 같이 성능이 크게 개선되는 것을 확인할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/29.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-CNN의 문제점과 개선된 알고리즘 SPPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-CNN은 region에 기반한 CNN feature를 사용하여 detection 성능을 크게 개선하였지만, 아무래도 처음 발표된 방식이라서 아래와 같은 문제점을 갖고 있다.\n",
    "- AlexNet의 구조를 그대로 사용하였기 때문에 입력 이미지 크기를 강제로 224x224 크기로 맞추기 위해 warping이나 crop을 사용했는데 이로 인한 이미지 변형이나 <br /> \n",
    "crop으로 인한 손실로 인해, 성능 저하가 일어날 수 있는 요인이 존재.\n",
    "- 2000여개에 이르는 region proposal에 대해 순차적으로 CNN을 수행해야 하기 때문에 학습이나 실제 run time이 긴 문제.\n",
    "- 사용하는 알고리즘이 특히, region proposal이나, SVM 튜닝 등이 GPU 사용에 적합하지 않다는 점."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 문제는 SPPNet(Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition) 개발자들에 의해 상당 부분이 극복이 된다. <br />\n",
    "SPPNet은 마이크로소프트의 북경연구소에 근무하는 Kaiming He 등에 의해서 개발이 되었으며, R-CNN의 문제점을 파고 든 결과라고 보인다. <br /> \n",
    "먼저 주목한 점은 AlexNet 중 convolutional 레이어 부분은 어차피 sliding window 방식을 사용하기 때문에 이미지의 크기에 영향을 받지 않으며, <br /> \n",
    "뒷 단에 오는 FC 레이어 부분만 이미지의 크기에 영향을 받는다는 것이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/30.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 SPPNet 논문에 나오는 그림으로 crop이나 warping을 하면 위 그림처럼 왜곡이나 손실이 발생한다. <br /> \n",
    "이 문제는 위 그림처럼 convolutional 레이어 다음에 spatial pyramid pooling 레이어를 두고 이 단계에서 pyramid 연산을 통해 입력 이미지의 크기에 대응할 수 있게 되면, <br /> \n",
    "굳이 crop/warp를 사용하지 않아도 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPPNet은 BoW(Bag-of-Words) 개념을 사용한다. <br /> \n",
    "BoW란 특정 개체를 분류하는데 굵고 강한 특징에 의존하는 대신에서 작은 여러 개의 특징을 사용하면 개체를 잘 구별할 수 있다는 사실에 기반한다. <br />\n",
    "AlexNet이나 ZFNet과 같은 기존 뉴럴 네트워크의 입력 이미지의 크기가 고정이 되는 이유는 convolutional 레이어는 이미지의 크기에 영향을 받지 않지 않지만 <br /> \n",
    "FC 레이어가 입력 이미지의 크기에 제한을 받기 때문이다. <br />\n",
    "SPPNet 설계자들은 BoW 개념처럼 여러 단계의 피라미드 레벨에서 오는 자잘한 feature들을 FC 레이어의 입력으로 사용하고, <br /> \n",
    "피라미드의 출력을 이미지의 크기에 관계없이 사전에 미리 정하면 더 이상 이미지의 크기에 제한을 받지 않게 된다는 점에 주목을 하였다. <br />\n",
    "그래서 기존 ZFNet과 같은 뉴럴 네트워크의 최종 convolutional 레이어를 pyramid pooling 레이어로 변환을 시키고, 최종 피라미드 레이어에서는 직전 convolutional 레이어의 <br /> \n",
    "결과를 여러 단계의 피라미드로 나눈다. 예를 들면, 1단계는 이미지 전체를 커버할 수 있도록 1x1 pooling, 2단계는 이미지를 4개의 영역으로 구분한 2x2 pooling, <br /> \n",
    "3단계는 이미지를 9개의 영역으로 구분은 3x3 pooling 등 이미지를 spatial bin이라고 불리는 총 M개의 영역으로 나눈다. <br /> \n",
    "이렇게 얻어진 여러 단계의 결과는 각각을 concatenation 시킨 후 fully-connected layer의 입력으로 사용한다. <br />\n",
    "이 때 입력 feature map의 갯수가 k개 라면, 앞서 구한 M개의 벡터가 kM개의 차원으로 존재하는 셈이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPPNet이 R-CNN에 비해 갖는 또 다른 장점 중 하나는 R-CNN은 각각의 후보 window에 대해 crop/warp를 한 후 CNN 과정을 전부 거치지만, <br /> SPPNet에서는 이미지 크기에 영향을 받지 않기 때문에 전체 이미지에 대해 딱 1번 convolution 레이어를 거친 후 해당 window에 대하여 SPP를 수행한 후에 <br />\n",
    "이 후 과정들을 거치기 때문에, 가장 시간이 오래 걸리는 convolution 과정을 건너 뛸 수가 있어 성능이 24~102배 정도 빠르다. <br />\n",
    "SPPNet은 2014년 ILSVRC에 참여를 하였으며, detection 부분에서는 GoogLeNet에 이어 2위를 차지했으며, classification 분야에서는 3위를 차지하였다. <br />\n",
    "다음 그림은 SPPNet의 구조를 보여준다. AlexNet의 5번째 convolutional layer 다음에 SPP layer가 위치를 하며, 이후에 FC 레이어가 오는 구조를 취한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/31.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet의 detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet에서도 detection 구조에 대해서는 자세한 언급이 없다. 다만 R-CNN과 비슷한 구조를 사용하고 있으며, 인셉션 모델을 region classifier로 사용한다. <br /> \n",
    "그리고 region proposal 단계에서는 selective search와 multi-box prediction을 혼합해서 사용한다. <br /> \n",
    "False positive의 수를 줄이기 위해 super-pixel의 크기를 2배 증가시켰으며, 이를 통해 region proposal의 수를 1/2로 줄였으며, <br /> \n",
    "이렇게 효과적으로 region proposal의 개수를 줄임으로써 mAP가 1%를 개선하였다. <br />\n",
    "학습 시간의 문제로 bounding-box regression 은 적용하지 않았지만 성능은 R-CNN에 비해 훨씬 좋은 결과를 얻을 수 있었다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
