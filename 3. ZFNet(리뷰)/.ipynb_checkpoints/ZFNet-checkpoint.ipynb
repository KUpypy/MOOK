{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZFNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN, 특히 레이어가 여러 개인 DNN의 경우 어떤 원리로 좋은 결과를 낼 수 있는지, CNN의 구조를 결정하는 하이퍼 파라미터는 어떻게 설정할 것인지 <br />\n",
    "또는 개발된 아키텍처가 과연 최적인지 판별하는 것은 매우 어려운 문제이다. <br />\n",
    "좋은 결과를 얻었지만 이론적으로 설명하기 어렵고, 최적의 구조인지도 확신할 수 없는 상황이기 때문에 무언가 CNN을 잘 이해할 수 있는 수단이 필요했으며, <br />\n",
    "Matthew Zeiler는 이것을 Visualizing 기법을 사용하여 해결하려고 하였다. <br />\n",
    "이와 관련된 주요 개념은 2011년에 이미 발표를 하였지만, 2012년 AlexNet의 결과가 Reference로 만들어지자, 2013년 논문 발표를 통해 자신의 연구가 효과적임을 입증하였다. <br />\n",
    "이후 많은 연구자들에 의해 많은 Visualizing 변형 기법들이 나오게 되고 이 중 가장 많이 사용되는 2가지를 소개하면 아래와 같다. <br />\n",
    "- Visualzing Convolutional Neural Networks for Image Classification (2015, Danel Brukner)\n",
    "- Delving Deep into Convolutional Nets (2014, Ken Chatfield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ZFNet은 특정 구조를 가리키는 개념이 아니라 CNN을 보다 잘 이해하기위한 기법을 가리키는 개념으로 이해하는 것이 좋다. <br />\n",
    "실제 논문에서도 새로운 구조를 발표했다기 보다는 AlexNet의 기본 구조를 자신들의 visualizing 기법을 통해 개선할 수 있을음 보여주었다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deconvolution을 이용한 Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN의 동작을 잘 이해하려면, 중간 레이어에서 feature의 activity가 어떻게 되는지 알 수 잇어야 한다. <br />\n",
    "그런데 중간 레이어의 동작을 집적 보기는 어려우니 이 activity들을 다시 입력 이미지 공간에 mapping시키는 방법이 바로 visualizing 기법의 핵심이다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 구조는 여러 개의 Conv 레이어를 기반으로 하고 있는데, 중간 단계 레이어에서는 이전 feature map으로 부터, 그것에 대하여 convolution을 수행하여 <br />\n",
    "현 단계의 feature map을 만들어내고 ReLU를 통해 활성화 시킨 후, pooling을 수행하는 아래의 그림과 같은 방식을 취한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정 feature의 activity가 입력 이미지에서 어떻게 mapping 되는지를 이해하려면, 위 과정을 역(reverse)으로 수행하면 된다. <br />\n",
    "하지만 여기에서 가장 문제가 되는 부분이 max pooling에 대한 역(reverse)을 구하는 것이다. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling 단계는 주변에서 가장 강한 자극만을 다음 레이어로 전달하기 때문에, 역방향으로 진행을 할 때에는 어떤 위치에 있는 신어가 가장 강한 신호였는지 <br />\n",
    "파악할 수 있는 방법이 없다. 여기에서 ZFNet 개발자는 Switch라고 불리는 개념을 생각해냈다. <br />\n",
    "Switch는 가장 강한 자극의 위치 정보를 갖고 있는 일종의 꼬리표라고 보면 된다. <br />\n",
    "역으로 unpooling을 수행할 때는 switch 정보를 할용하여 가장 강한 자극의 위치로 정확하게 찾아갈 수 있다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 가장 강한 자극 이외의 값들도 존재하였지만, max pooling을 수행하는 과정에서 사라졌기 때문에 이 방법의 단점은 가장 강한 자극(feature)에 대한 영향만 파악이 가능하다. <br />\n",
    "아래의 그림을 보면, 녹색 화살표가 가리키는 자극은 주변보다 강하지 않기 때문에 max pooling을 수행하면서 오른쪽 빨간색 화살표처럼 사라진다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 과정은 ReLU의 역과정을 수행하여야 하는데, ReLU에서 음의 값들은 0 으로 정류(recitify)되고 0 이상의 값들은 그대로 pass 시키기 때문에, <br />\n",
    "역과정을 수행할 때 양의 값들은 문제가 없지만 0 이하의 값들은 복원할 방법이 없다. <br />\n",
    "그렇지만 이들의 연구 결과에 따르면 이에 대한 영향은 미미하다고 보고하고있다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 과정은 filter에 대한 inverse filter 연산을 수행하는 것인데, 이 과정은 가역이 가능한 과정이므로 문제가 되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 Deconvolution을 수행하면, 정확하게 입력과 같은 상태로 mapping이 되는 것은 아니지만, 가장 강한 자극(feature)가 입력 이미지에 어떻게 mapping 되는지를 <br />\n",
    "확인 할 수 있고, 한 걸음 더 나아가 어떤 구조가 최적의 구조인지 더 잘 결정할 수 있도록 도움을 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터를 이용하여 training을 마치게 되면, 각 레이어에서 파라미터가 결정된다. AlexNet처럼 5개의 Conv 레이어를 갖는 CNN 구조를 생각해보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of features in a fully trained model. For layers 2-5 we show the top 9 activations in a random subset\n",
    "of feature maps across the validation data, projected down to pixel space using our deconvolutional network approach.\n",
    "Our reconstructions are not samples from the model: they are reconstructed patterns from the validation set that cause\n",
    "high activations in a given feature map. For each feature map we also show the corresponding image patches. Note:\n",
    "(i) the the strong grouping within each feature map, (ii) greater invariance at higher layers and (iii) exaggeration of\n",
    "discriminative parts of the image, e.g. eyes and noses of dogs (layer 4, row 1, cols 1). Best viewed in electronic form.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/3.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "위의 그림은 레이어1과 레이어2를 보여준다. 주로 이미지의 corner나 edge, 혹은 컬러와 같은 low level feature들이 나타난다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "다음은 레이어 3에 대한 그림이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/4.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이 그림을 보면, 레이어 1과 레이어 2에 비해 좀 더 복잡한(상위 수준)의 invariance가 시각화되거나 혹은 비슷한 texture에 대한 feature를 보여준다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "레이어 4에서는 사물이나 개체의 일부분을 볼 수 있으며, 아래 그림의 경우 개의 얼굴(1열, 1행)이나 새의 다리(4열 1행)등을 볼 수 있다. <br />\n",
    "레이어 5에서는 위치나 pose의 변화등 자세한 변화를 포함한 사물이나 개체의 전부를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/5.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이렇게 visualizing을 수행하게 되면, 각각의 intermediate 레이어에서의 feature map이 원래의 이미지에서 어떤 영역에 해당하는지 알 수 있다. <br />\n",
    "특정 레이어에서 얻어진 feature map들이 고르게 분포되어 있는지 혹은 특정 feature 쪽으로 쏠려 있지는 않은지, 개체의 pose나 기타 invariance등을 잘 반영하는지 등을 <br />\n",
    "파악할 수 있기 때문에 학습이 잘 진행되었는지 판단할 수 있고 또한 local feature들이 layer가 올라감에 따라 global feature들로 바뀌는 것에 대해서도 이해 할 수 있다. <br />\n",
    "Visualization 기법은 feature map들을 살펴봄으로서 해당 CNN 아키텍처의 적합성 여부를 판단하는데 좋은 방법이 될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지의 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 우리가 임의의 이미지들을 자동 분류(Auto-tagging)하고 싶다면, 어떻게 해야 할까? <br />\n",
    "답은 이미지를 분석하여, 그 이미지를 대표할 수 있는 representative를 끄집어 내야 한다. <br />\n",
    "이미지를 대표할 수 있는 특징은 크게 3가지로 분류 가능하다. <br />\n",
    "- Low level feature: edge, corner, color\n",
    "- Mid level feature: edge junction\n",
    "- High level feature: object의 일부분 혹은 object 전체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 대표할 수 있는 feature로 흔히 이미지의 경계(edge)를 꼽을 수 있다. <br />\n",
    "아래의 그림은 Zeiler가 본인의 논문에서 사용한 그림이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 위의 그림을 보면 서로 다른 개가 나오는데 이것을 edge에 기반한 feature만 제대로 분류가 가능할까? <br />\n",
    "또한 다양한 각도에서 바라보는 자전거를 잘 분류해 낼 수 있을까? <br />\n",
    "SIFT 기반의 결과와 CNN을 사용한 결과는 10%이상의 성능 차이를 보였으며, 2013년 Zelier의 Clarifai는 다시 2012년 결과를 5%이상 개선하였기 때문에 <br />\n",
    "CNN과 low level feature 기반의 알고리즘을 비교하는 것은 의미가 없다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT(Scale Invariant Feature Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT는 크기의 변화나 회전에 영향을 받지 않는 이미지의 feature을 검출하고 그것으로 괜찮은 성능을 보이는 알고리즘이다. <br />\n",
    "SIFT는 크게 4개의 단계를 거쳐 특정 object를 대표할 수 있는 descriptor를 생성한다. <br />\n",
    "- Scale space extrema 찾기\n",
    "- Key point localization & Filtering\n",
    "- Orientation 할당\n",
    "- Descriptor 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 그림은 SIFT 알고리즘을 설명할 때, 가장 많이 사용되는 그림이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에서 단순히 경계를 검출하게되면, 노이즈를 포함한 의미없는 edge도 많이 검출하기 떄문에 먼저 Gaussian filter를 여러 단계로 적용(&\\sigma& 값을 조절)하여 <br /> \n",
    "이미지를 blur시키고 이들간의 차이를 구하면, 노이즈에 강한 feature가 얻어진다. (DoG: Difference of Gaussian) <br />\n",
    "이렇게 현재 scale에 대한 작업을 마치면, scale 변화에 대응이 가능하도록 이미지의의 scale을 바꾸어가면서 동일한 작업을 수행하여, 서로 다른 scale에서 노이즈에 <br /> \n",
    "강한 feature(extrema)를 구한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DoG를 통해 비교적 노이즈에 강한 extrema를 추출했을지라도 아직 쓸모없는 정보들이 많이 포함되어 있기 때문에 이것을 key point localization과 filtering을 통해 <br /> \n",
    "의미있는 정보들만 추출해내야한다. 다음은 gradien의 방향에 따른 히스토그램을 구한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림은 4x4 그라디언트 window에서 8개의 방향으로 4x4x8=128 총 128차원의 descriptor을 만들고 이를 비교하는 방식으로 이미지의 정합성 여부를 판단한다. <br />\n",
    "결과적으로 보면, 이미지의 mid나 high level의 특징을 보는 대신, low level gradient가 scale space에서 어느 방향으로 향하는 것인가에 대해서만 살펴보기 때문에 <br />\n",
    "한계를 가질 수 밖에 없다. 조명의 변화나 이미지를 얻는 과정에서 saturation되는 부분에 따라서도 많은 영향을 받게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CNN과 Feature Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CNN이 SIFT등의 기존 알고리즘과 다른 점은 여러 단계의 convolution과 pooling을 거치면서 mid 와 high level feature을 이용한다는 점이다. <br />\n",
    "그리고 처리 해야할 이미지의 형태나 내용과 관계없이 동일한 방식을 따르는 것이 아니라, 데이터 셋에 대한 학습이 진행되면서 convolution filter의 파라미터들이 <br />\n",
    "최적화된다는 점이 다르다. 하지만 CNN에서도 최적의 결과를 얻기위해서는 적절한 하이퍼 파라미터의 설정이 필요하다. <br />\n",
    "Feature Visualizing 기법은 CNN에서 최적의 구조를 결정하는데 크게 도움이 되는 수단을 제공한다. <br />\n",
    "하지만 만약 이러한 기법이 없다면, 경험이나 직관에 의존하여 구조를 결정해야한다. <br />\n",
    "하지만 feature visualizing 기법을 이용하면, 중간 단계의 convolution에서 적절한 feature들이 추출되고 있는지를 확인할 수 있으며 <br />\n",
    "이를 통해 현재 설정되어있는 하이퍼 파라미터의 조합이 최적의 결과인지도 어느정도 확인할 수 있다. <br />\n",
    "논문에 있는 실험 결과를 살펴보자.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/9.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "위의 그림에서 (a)는 Krizhevsky의 논문에 기반하여 얻은 첫 번째 레이어의 feature map들을 시각화한 것이며, 이를 통해 첫 번째 레이어의 feature map들이 특정 feature에 편중되어 나타나는 것을 알 수 있다. <br />\n",
    "이는 filter들을 통해 끄집어 낼 수 있는 대표성이 제한된다는 것을 의미한다. <br />\n",
    "(b)는 Zeiler가 좀 더 개선된 결과를 얻어내기 위해 기존 아키텍처의 첫 번째 레이어에서 filter의 크기과 stride를 바꾸어 개선한 것이다. <br />\n",
    "(b)는 (a)보다 훨씬 다양한 feature를 검출할 수 있다는 것을 확인할 수 있다. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 그림에서 (c)는 krizhevsky 논문의 두 번째 레이어를 visualization 기법으로 본 결과이며, (d)는 ZFNet의 결과이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/10.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(d)는 (c)보다 훨씬 선명하고, aliasing artifact도 없음을 알 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이처럼, feature visualization 기법을 잘 활용하면, 최적의 아키텍처를 결정하는데 큰 도움을 받을 수 있다. <br />\n",
    "좋은 feature는 topology나 scale의 변화에 무관해야한다. <br />\n",
    "다섯 번째 레이어에서 살펴보면,\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "여러 형태 여러 위치에서 개(dog)를 잘 검출할 수 있는 것을 알 수 있다. <br />\n",
    "이 또한 visualization 기법을 통해 쉽게 확인이 가능하며, 마지막 convolution 레이어에서는 edge나 edge의 junction과 같은 low level feature가 아니라 <br />\n",
    "개의 얼굴 또는 자전거 바퀴와 같은 object의 일부분이나 전체를 효과적으로 검출할 수 있다는 것을 확인하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet VS ZFNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet의 구조는 아래의 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet은 2개의 GPU를 활용하는 구조를 취하고 있으며, 필요에 따라 inter GPU/intra GPU 통신 구조를 취하고 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeiler는 visualization 기법을 활용하여, AlexNet의 구조의 문제점을 지적한다. <br />\n",
    "AlexNet의 설계자들은 2개의 GPU를 사용하는 방법에 대하여 많은 고민을 했을 것이다. 결국 Zeiler에 의해 AlexNet의 구조가 최적이 아니었음을 증명된다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/13.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림의 (b)와 (d)는 AlexNet의 첫 번째 레이어와 두 번째 레이어의 feature map들을 보여주는 것이고 (c)와 (e)는 ZFNet의 결과를 보여주는 것이다. <br />\n",
    "AlexNet은 feature map들이 일부 feature에 몰리는 현상이나 aliasing 문제가 발생하기도 하였지만 ZFNet는 다양한 feature가 고르게 나타나는 것을 볼 수 있다. <br />\n",
    "당연히 AlexNet보다 ZFNet을 이용한 결과가 좋게 나온다. <br />\n",
    "아래의 그림은 ZFNet의 구조이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/14.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZFNet도 AlexNet과 동일하게 GTX580 GPU를 사용하였다. 하지만 AlexNet과는 달리 1개의 GPU를 사용하였다. <br />\n",
    "ILSVRC 이미지를 학습 할 때 70 epoch에서 멈췄으며, 1개의 GPU를 사용하여 총 12일 동안 학습을 시켰다고 한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZFNet은 자신들의 feature visualization 기법의 효과를 보이는 것에 집중하였기 때문에 전반적으로 AlexNet의 구조를 그대로 가져왔다. <br />\n",
    "단, 첫 번째와 두 번째 레이어의 구조는 기존 AlexNet과 다르다는 것을 알 수 있다. 아래는 이를 확대한 그림이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/15.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet은 첫 번째 convolution 레이어에 11x11 크기의 filter에 stride를 4로 정하였는데, ZFNet은 filter의 크기를 7x7로 하고 stride를 2로 정하였다. <br />\n",
    "그렇게 얻은 110x110 크기의 96개의 feature map에 대하여 3x3 overlapped max pooling을 수행하였으며, stride를 2로 하여 출력으로 <br />\n",
    "55x55 크기의 256개의 feature map을 얻었다. <br />\n",
    "이는 기존 AlexNet과 확연하게 다른 부분이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 filter의 크기를 크게하고 stride를 크게하는 방식은, filter의 크기를 작게 하고 stride의 크기를 작게하는 방식보다 결과가 좋지 못하다. <br />\n",
    "이를 Zeiler는 feature visualization을 통해 밝혀냈다. 그리고 더 좋은 결과를 얻을 수 있도록 AlexNet의 구조를 변형시켰다. <br />\n",
    "이후 과정은 AlexNet과 거의 같다. 하지만 AlexNet의 경우처럼, 2개의 GPU에 mapping 시킥 위해 인위적으로 네트워크를 나누어서 처리하는 부분을 제거하였다. <br />\n",
    "ZFNet을 통하여 증명하였듯이, AlexNet 구조처럼 2개의 GPU에서 다른 처리를 시도하는 경우는 다른 어떤 연구에서도 찾아 볼 수 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization을 통해 확인할 수 있는 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 레이어 별로 feature을 습득하는 시간이 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 그림에서 볼 수 있는 것처럼\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/16.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 단의 레이어들은 몇 번의 학습 epoch만에 feature들이 어느정도 수렴하는 것을 볼 수 있다. <br />\n",
    "하지만, 뒤쪽 레이어로 갈수록 feature을 습득하는데 오랜 시간이 걸리며, 적어도 40~50 epoch 이상이 되어야 feature들이 보이기 시작한다. <br />\n",
    "이 점은 CNN 학습의 epoch 수를 결정할 때 참고하면 좋다. 실제로 ZFNet을 학습할 때는 70 epoch까지 진행을 하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 크기, 위치 및 회전 변화에 따른 invariance가 향상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 그림은 크기, 위치 및 회전 이동에 사용할 테스트 이미지들이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/17.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 단의 레이어에서는 작은 변화에 대해서도 눈에 띌 정도의 변화가 있었지만, 뒤로 갈수록 invariance를 얻을 수 있음을 연구를 통해 알아내었다. <br />\n",
    "또한 위치 이동과 크기 변화에 대해서는 거의 linear한 특성을 얻을 수 있었다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 그림은 위 이동에 대한 결과이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/18.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험에 사용한 5 종류의 이미지들에 대하여 각 레이어에서의 유클리드 거리를 구한 것을 보여준다. <br />\n",
    "레이어가 뒤로 갈수록 invariance한 성질이 있음을 알 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개체의 정확한 위치를 알아냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN을 통해 이미지 속에 있는 object(개체)의 위치까지 파악 가능하다. 또한 주변에 있는 것들과의 관계 속에서 개체를 알아내는지의 여부도 실험을 통해 파악할 수 있다. <br />\n",
    "이를 확인하기 위해, 개체의 일부 영역을 사각형 형태의 회색 칠을 하고 결과를 확인하였다. <br />\n",
    "아래 그림을 통해 결과를 확인 할 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/19.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Three test examples where we systematically cover up different portions of the scene with a gray square (1st\n",
    "column) and see how the top (layer 5) feature maps ((b) & (c)) and classifier output ((d) & (e)) changes. (b): for each\n",
    "position of the gray scale, we record the total activation in one layer 5 feature map (the one with the strongest response\n",
    "in the unoccluded image). (c): a visualization of this feature map projected down into the input image (black square),\n",
    "along with visualizations of this map from other images. The first row example shows the strongest feature to be the\n",
    "dog’s face. When this is covered-up the activity in the feature map decreases (blue area in (b)). (d): a map of correct\n",
    "class probability, as a function of the position of the gray square. E.g. when the dog’s face is obscured, the probability\n",
    "for “pomeranian” drops significantly. (e): the most probable label as a function of occluder position. E.g. in the 1st row,\n",
    "for most locations it is “pomeranian”, but if the dog’s face is obscured but not the ball, then it predicts “tennis ball”. In\n",
    "the 2nd example, text on the car is the strongest feature in layer 5, but the classifier is most sensitive to the wheel. The\n",
    "3rd example contains multiple objects. The strongest feature in layer 5 picks out the faces, but the classifier is sensitive\n",
    "to the dog (blue region in (d)), since it uses multiple feature maps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
